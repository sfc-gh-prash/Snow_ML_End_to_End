{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de2aef0-1e86-4b22-adf3-97a1c4382217",
   "metadata": {
    "collapsed": false,
    "name": "End_to_End_AI_Project"
   },
   "source": [
    "### End to End ML In Snowflake\n",
    "\n",
    "1. Data Generation \n",
    "2. Basic EDA \n",
    "3.Feature Engineering \n",
    "     creating new features - \n",
    "     using cortex as well to create new features in pipleine logs as well (cortex for summarizing )\n",
    "4.Use Feature Store to track engineered features\n",
    "    - Store feature defintions in feature store for reproducible computation of ML features\n",
    "2. Train two SnowML & Sciktleanrn model \n",
    "    - Xgboost with tree booster, linear booster\n",
    "3. Register both models in Snowflake model registry\n",
    "    - Explore model registry capabilities such as metadata tracking, inference, and explainability\n",
    "4. Set up Model Monitor to track 1 year of predicted and actual loan repayments\n",
    "    - Compute performance metrics such a F1, Precision, Recall\n",
    "    - Inspect model drift (i.e. how much has the average predicted repayment rate changed day-to-day)\n",
    "    - Compare models side-by-side to understand which model should be used in production and deploying the model \n",
    "    - Identify and understand data issues\n",
    "5. Track data and model lineage throughout\n",
    "    - View and understand\n",
    "        * The origin of the data used for computed features\n",
    "        * The data used for model training\n",
    "        * The available model versions being monitored\n",
    "6. finally building a streamlit app leveraging cortex analyst to get answers from the data \n",
    "\n",
    "#### Flow of the presentation \n",
    "\n",
    "- Data Generation, \n",
    "Fraud detection \n",
    "- EDA \n",
    "- Feature engineering using multiple feature and cortex as well \n",
    "- Use Feature Store to track engineered features\n",
    "- Train & evaluate models (XGBoost, RF, LR)\n",
    "- Model deploymeent & monitoring - Low latency less < 1 ,s ( SPCS & Warehouse )  ####clarify\n",
    "- App \n",
    "- cortex analyst to find answers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c876b-d5f9-4c61-855d-111e111ed26d",
   "metadata": {
    "language": "python",
    "name": "Importing_libraries"
   },
   "outputs": [],
   "source": [
    "# Standard Python Libraries\n",
    "import sys\n",
    "import json\n",
    "import warnings\n",
    "from datetime import timedelta\n",
    "\n",
    "# Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "# Snowpark Core\n",
    "from snowflake.snowpark import Session, DataFrame\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark.version import VERSION\n",
    "import snowflake.snowpark.functions as F\n",
    "from snowflake.snowpark.exceptions import SnowparkSessionException\n",
    "from snowflake.snowpark.functions import (sproc, col, dayname, \n",
    "                              to_timestamp,min, max,split\n",
    ")\n",
    "\n",
    "\n",
    "from snowflake.snowpark import types as T\n",
    "from snowflake.snowpark.window import Window\n",
    "\n",
    "# Snowpark ML\n",
    "from snowflake.ml.modeling.impute import SimpleImputer\n",
    "from snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from snowflake.ml.modeling.pipeline import Pipeline\n",
    "from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from snowflake.ml.modeling.model_selection import GridSearchCV\n",
    "from snowflake.ml.modeling.metrics import mean_absolute_percentage_error\n",
    "from snowflake.ml.registry import Registry\n",
    "\n",
    "# Snowflake Feature Store\n",
    "from snowflake.ml.feature_store import (\n",
    "    FeatureStore, FeatureView, Entity, CreationMode, setup_feature_store\n",
    ")\n",
    "\n",
    "# Snowflake Task API\n",
    "from snowflake.core import Root\n",
    "from snowflake.core.database import Database\n",
    "from snowflake.core.schema import Schema\n",
    "from snowflake.core.warehouse import Warehouse\n",
    "from snowflake.core.task import StoredProcedureCall\n",
    "from snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation\n",
    "from snowflake.core._common import CreateMode\n",
    "\n",
    "# Streamlit\n",
    "import streamlit as st\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create the Model Registry and register your initial model\n",
    "from snowflake.ml.registry import Registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfd9091-b782-450c-bbf2-b5a129b115af",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "session_start"
   },
   "outputs": [],
   "source": [
    "# Create Snowflake Session object\n",
    "session = get_active_session()\n",
    "session.sql_simplifier_enabled = True\n",
    "\n",
    "snowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\n",
    "snowpark_version = VERSION\n",
    "\n",
    "# Current Environment Details\n",
    "print('\\nConnection Established with the following parameters:')\n",
    "print('User                        : {}'.format(snowflake_environment[0][0]))\n",
    "print('Role                        : {}'.format(session.get_current_role()))\n",
    "print('Database                    : {}'.format(session.get_current_database()))\n",
    "print('Schema                      : {}'.format(session.get_current_schema()))\n",
    "print('Warehouse                   : {}'.format(session.get_current_warehouse()))\n",
    "print('Snowflake version           : {}'.format(snowflake_environment[0][1]))\n",
    "print('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a62b34-28b6-429b-90c8-ffc9e6f6a2ab",
   "metadata": {
    "collapsed": false,
    "name": "Data_Generation"
   },
   "source": [
    "Data Generation Script TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd2433-a352-47fb-879e-e8a9cfcf4535",
   "metadata": {
    "collapsed": false,
    "name": "EDA"
   },
   "source": [
    "- Do some basics EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80563a67-0bba-45ef-8e59-fa9deb7e71fc",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "transaction_df"
   },
   "outputs": [],
   "source": [
    "select * from transactions limit 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f93abc-053b-417b-9317-7c3db63e9776",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "customer_df"
   },
   "outputs": [],
   "source": [
    "select * from customer_complaints limit 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2164b6c3-ce3f-45e1-97af-40f683d11c3d",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "code_snippet"
   },
   "outputs": [],
   "source": [
    "--drop table fraud_analysis;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f88cbc-8753-4871-b791-5520f8c36ed2",
   "metadata": {
    "collapsed": false,
    "name": "Merging_dataset"
   },
   "source": [
    "Merge two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319d9c5-e621-4e8d-87bc-ba72f7203638",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "Merge_two_datasets"
   },
   "outputs": [],
   "source": [
    "# Create the table using SQL\n",
    "session.sql(\"\"\"\n",
    "CREATE OR REPLACE TABLE fraud_analysis AS\n",
    "SELECT \n",
    "    t.transaction_id, \n",
    "    t.customer_id, \n",
    "    t.transaction_amount, \n",
    "    t.is_fraud, \n",
    "    t.merchant_category,\n",
    "    t.device_type,\n",
    "    t.location,\n",
    "    t.transaction_time,\n",
    "    c.complaint_text, \n",
    "    c.keywords,\n",
    "    c.complaint_time\n",
    "FROM transactions t\n",
    "LEFT JOIN customer_complaints c\n",
    "ON t.customer_id = c.customer_id\n",
    "\"\"\").collect()\n",
    "\n",
    "# Create a Snowpark DataFrame from the newly created table\n",
    "df = session.table(\"fraud_analysis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1043d068-7dee-49a2-9f7f-194d901858d2",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "fA_list"
   },
   "outputs": [],
   "source": [
    "select * from fraud_analysis limit 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac6d56-1d1e-456e-a9d5-a0ef6401b37e",
   "metadata": {
    "language": "python",
    "name": "df_show"
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc5a4ad-a867-4f58-b003-e3f38ad2fdc8",
   "metadata": {
    "collapsed": false,
    "name": "Adding_column_FE"
   },
   "source": [
    "- lets do feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37c879-ba74-4923-aec7-244e23586b7e",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "Adding_column"
   },
   "outputs": [],
   "source": [
    "-- ALTER TABLE fraud_analysis \n",
    "-- ADD COLUMN computed_sentiment STRING;\n",
    "-- UPDATE fraud_analysis \n",
    "-- SET computed_sentiment = SNOWFLAKE.CORTEX.SENTIMENT(complaint_text);\n",
    "-- SELECT complaint_text, computed_sentiment \n",
    "-- FROM fraud_analysis \n",
    "-- LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d005a7-b1d6-4910-b9fa-aa22e3d2410e",
   "metadata": {
    "collapsed": false,
    "name": "Feature_Store"
   },
   "source": [
    "### Create features with Feature Store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346a407-0b50-4183-9b88-0760c88eaae9",
   "metadata": {
    "collapsed": false,
    "name": "cf_withfeaturestore"
   },
   "source": [
    "Initialize Feature Store\n",
    "Let's first create a feature store client. With CREATE_IF_NOT_EXIST mode, it will try to create a new Feature Store schema and all necessary feature store metadata if it doesn't exist already. It is required for the first time to set up a Feature Store. Afterwards, you can use FAIL_IF_NOT_EXIST mode to connect to an existing Feature Store.\n",
    "\n",
    "Note that the database being used must already exist. Feature Store will NOT try to create the database even in CREATE_IF_NOT_EXIST mode.\n",
    "Generate cumulative behavioral metrics for users based on their transaction data, such as cumulative clicks and cumulative logins per hour. It involves the use of window functions and joins to combine and transform data from the CREDITCARD_TRANSACTIONS table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c231f3-cb38-42d1-82cd-01c3c7650cf9",
   "metadata": {
    "language": "python",
    "name": "see_timespan"
   },
   "outputs": [],
   "source": [
    "df.select(min('TRANSACTION_TIME'), max('TRANSACTION_TIME'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4a42d-0e07-4f86-83af-2f0c43d9eaf8",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "define_features"
   },
   "outputs": [],
   "source": [
    "#Create a dict with keys for feature names and values containing transform code\n",
    "from snowflake.snowpark.functions import call_udf\n",
    "feature_eng_dict = dict()\n",
    "feature_eng_dict[\"TRANSACTION_TIME\"] = to_timestamp(\"TRANSACTION_TIME\")\n",
    "feature_eng_dict[\"SENTIMENT_SCORE\"] = call_udf(\"SNOWFLAKE.CORTEX.SENTIMENT\", col(\"complaint_text\"))\n",
    "feature_eng_dict[\"TRANSACTION_DAY\"] = dayname(col(\"transaction_time\"))\n",
    "df = df.with_columns(feature_eng_dict.keys(), feature_eng_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c4cb1-b816-4867-85f8-fe1ecdea7f03",
   "metadata": {
    "language": "python",
    "name": "df_show_v"
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa62e2d-b2a2-4f7f-8e3b-30ef4f3b608f",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "df_explain"
   },
   "outputs": [],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e0bba9-1133-4e9e-8a6e-aa6dc2394c05",
   "metadata": {
    "collapsed": false,
    "name": "customertrans_entity"
   },
   "source": [
    "Creating Entities\n",
    "\n",
    "An entity is an abstraction over a set of primary keys used for looking up feature data. An Entity represents a real-world \"thing\" that has data associated with it. Below cell registers an entity for Customer and Transaction in Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd50344-d2fb-4ac8-adf0-78a6c3201ba2",
   "metadata": {
    "language": "python",
    "name": "define_feature_store"
   },
   "outputs": [],
   "source": [
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=session.get_current_database(), \n",
    "    name=session.get_current_schema(), \n",
    "    default_warehouse=session.get_current_warehouse(),\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a277df89-db77-4b9a-a924-7b1221ac475c",
   "metadata": {
    "language": "python",
    "name": "list_entities"
   },
   "outputs": [],
   "source": [
    "fs.list_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21b0b42-920f-4295-ac55-e1f2a928e734",
   "metadata": {
    "language": "python",
    "name": "load_or_register_entities"
   },
   "outputs": [],
   "source": [
    "# First try to retrieve an existing entity definition, if not define a new one and register\n",
    "try:\n",
    "    # Retrieve existing entity\n",
    "    customer_entity = fs.get_entity('CUSTOMER_ID_ENTITY') \n",
    "    print('Retrieved existing entity')\n",
    "except:\n",
    "    # Define new entity\n",
    "    customer_entity = Entity(\n",
    "        name = \"CUSTOMER_ID_ENTITY\",\n",
    "        join_keys = [\"CUSTOMER_ID\"],\n",
    "        desc = \"Features defined on a per customer level\")\n",
    "    \n",
    "    # Register\n",
    "    fs.register_entity(customer_entity)\n",
    "    print(\"Registered new entity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c788c-d46f-49e8-aebb-a2a78794465f",
   "metadata": {
    "language": "python",
    "name": "List_entities"
   },
   "outputs": [],
   "source": [
    "fs.list_entities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1be891f-3225-46de-b688-b1f1c31981a4",
   "metadata": {
    "language": "python",
    "name": "create_feature_df"
   },
   "outputs": [],
   "source": [
    "#Create a dataframe with just the ID, timestamp, and engineered features. We will use this to define our feature view\n",
    "feature_df = df.select([\"CUSTOMER_ID\"]+list(feature_eng_dict.keys()))\n",
    "feature_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19f2b0d-4fc2-41a4-959f-a0d549e0d413",
   "metadata": {
    "collapsed": false,
    "name": "Feature_Views"
   },
   "source": [
    "# Using Feature Views\n",
    "\n",
    "A feature view is a group of logically-related features that are refreshed on the same schedule. The FeatureView constructor accepts a Snowpark DataFrame that contains the feature generation logic. The provided DataFrame must contain the join_keys columns specified in the entities associated with the feature view. In this example we are using time-series data, so we will also specify the timestamp column name.\n",
    "\n",
    "Below cell creates a feature view for the customer features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d79ea2-1b91-4c60-98a8-f54940097562",
   "metadata": {
    "language": "python",
    "name": "feature_veiw_creation"
   },
   "outputs": [],
   "source": [
    "#define and register feature view\n",
    "fraud_fv = FeatureView(\n",
    "    name=\"FRAUD_FEATURES\",\n",
    "    entities=[customer_entity],\n",
    "    feature_df=feature_df,\n",
    "    timestamp_col=\"TRANSACTION_TIME\")\n",
    "\n",
    "fraud_fv = fs.register_feature_view(fraud_fv, version=\"1\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451304eb-77ee-4e5c-a9f9-d05b25da33aa",
   "metadata": {
    "language": "python",
    "name": "feature_view"
   },
   "outputs": [],
   "source": [
    "fraud_fv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7067c5-5048-4a55-a462-31da4d337a10",
   "metadata": {
    "language": "python",
    "name": "show_feature_views"
   },
   "outputs": [],
   "source": [
    "fs.list_feature_views()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90323264-df3d-4679-ab8b-1c9ea3961120",
   "metadata": {
    "collapsed": false,
    "name": "Writeup_featurestore"
   },
   "source": [
    "This completes the setup for the Database objects and Feature Store Producer workflow. The data and the features which have been generated is available for the consumer with appropritate privileges. Time to head on to the next notebook!\n",
    "Generating Datasets for Training\n",
    "We are now ready to generate our training set. We'll define a spine DataFrame to form the backbone of our generated dataset and pass it into FeatureStore.generate_dataset() along with our Feature Views.\n",
    "\n",
    "NOTE: The spine serves as a request template and specifies the entities, labels and timestamps (when applicable). The feature store then attaches feature values along the spine using an AS-OF join to efficiently combine and serve the relevant, point-in-time correct feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a5bcb9-6d3b-4e26-b868-928fd05997ac",
   "metadata": {
    "language": "python",
    "name": "DF_SHOW"
   },
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c927fae-33fa-4760-a383-310e5119468c",
   "metadata": {
    "language": "python",
    "name": "generate_dataset"
   },
   "outputs": [],
   "source": [
    "ds = fs.generate_dataset(\n",
    "    name=\"FRAUD_DETECTION_DATASET_V1\",\n",
    "    spine_df=df.drop(\"SENTIMENT_SCORE\", \"complaint_text\",\n",
    " \"TRANSACTION_DAY\",\"KEYWORDS\",\"COMPLAINT_TIME\"),\n",
    "    features=[fraud_fv],\n",
    "    spine_timestamp_col=\"TRANSACTION_TIME\",\n",
    "    spine_label_cols=[\"IS_FRAUD\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6a19d4-86ac-4118-b451-1014b0f5cf5c",
   "metadata": {
    "language": "python",
    "name": "convert_dataset_to_snowpark_and_pandas"
   },
   "outputs": [],
   "source": [
    "ds_sp = ds.read.to_snowpark_dataframe()\n",
    "ds_sp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b22898-ddf1-41dc-ad03-b5a3ceaf9598",
   "metadata": {
    "language": "python",
    "name": "one_hot_encoding"
   },
   "outputs": [],
   "source": [
    "from snowflake.snowpark.types import StringType\n",
    "import snowflake.ml.modeling.preprocessing as snowml\n",
    "\n",
    "# Select categorical columns (columns with StringType), excluding CUSTOMER_ID and TRANSACTION_ID\n",
    "OHE_COLS = [col.name for col in ds_sp.schema.fields\n",
    "            if isinstance(col.datatype, StringType)\n",
    "            and col.name not in ('CUSTOMER_ID', 'TRANSACTION_ID')]\n",
    "\n",
    "# Create output column names for one-hot encoding\n",
    "OHE_POST_COLS = [i + \"_OHE\" for i in OHE_COLS]\n",
    "\n",
    "# Encode categorical columns to numeric columns using OneHotEncoder\n",
    "snowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols=OHE_POST_COLS, drop_input_cols=True)\n",
    "\n",
    "# Fit and transform the dataset\n",
    "ds_sp_ohe = snowml_ohe.fit(ds_sp).transform(ds_sp)\n",
    "\n",
    "# Print the resulting column names\n",
    "ds_sp_ohe.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60c1df-fda5-4990-a3d8-edd6b3e91f18",
   "metadata": {
    "collapsed": false,
    "name": "train_snowmlmodel"
   },
   "source": [
    "### training the model trying snowml model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96297cac-0a0e-4bdc-8be6-162d8016ad0c",
   "metadata": {
    "language": "python",
    "name": "train_test_split"
   },
   "outputs": [],
   "source": [
    "train, test = ds_sp_ohe.random_split(weights=[0.70, 0.30], seed=216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd3338d-2cdf-4833-a699-17f6de8b1b96",
   "metadata": {
    "language": "python",
    "name": "fill_na"
   },
   "outputs": [],
   "source": [
    "train = train.fillna(0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f10cb5-867c-4dd0-a01e-d9ef12e80efb",
   "metadata": {
    "language": "python",
    "name": "train_show"
   },
   "outputs": [],
   "source": [
    "train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ce9ce4-89c9-4177-8c1e-d41c9895289e",
   "metadata": {
    "language": "python",
    "name": "test_show"
   },
   "outputs": [],
   "source": [
    "test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b232775e-7e8b-4cc9-8002-d20b35e357af",
   "metadata": {
    "language": "python",
    "name": "define_themodel"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.xgboost import XGBRegressor, XGBClassifier\n",
    "\n",
    "snow_xgb_tree = XGBClassifier(\n",
    "    input_cols=train.drop([\"IS_FRAUD\", \"TRANSACTION_TIME\", \"CUSTOMER_ID\",'TRANSACTION_ID']).columns,\n",
    "    label_cols=train.select(\"IS_FRAUD\").columns,\n",
    "    output_cols=\"FRAUD_PREDICTION\",\n",
    "    learning_rate = 0.75,\n",
    "    ##tree_method=\"exact\",\n",
    "    ##n_estimators=5,\n",
    "    booster = 'gbtree'\n",
    ")\n",
    "\n",
    "# snow_xgb_linear = XGBClassifier(\n",
    "#     input_cols=train.drop([\"IS_FRAUD\", \"TRANSACTION_TIME\", \"CUSTOMER_ID\", \"MORTGAGERESPONSE\"]).columns,\n",
    "#     label_cols=train.select(\"IS_FRAUD\").columns,\n",
    "#     output_cols=\"FRAUD_PREDICTION\",\n",
    "#     # tree_method=\"exact\",\n",
    "#     # n_estimators=10,\n",
    "#     booster = 'gblinear'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce24c71-dfdf-4f9c-b43d-217e0216504a",
   "metadata": {
    "language": "python",
    "name": "train_tree"
   },
   "outputs": [],
   "source": [
    "\n",
    "snow_xgb_tree.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465bab39-0b7d-4d9d-a485-9be40ecf1c34",
   "metadata": {
    "language": "python",
    "name": "train_linear"
   },
   "outputs": [],
   "source": [
    "# snow_xgb_linear.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2afb56-86d6-46c4-aec1-09555869ba46",
   "metadata": {
    "language": "python",
    "name": "compute_predictions_and_perf_metrics"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "test_preds_tree = snow_xgb_tree.predict(test).select([\"IS_FRAUD\", \"FRAUD_PREDICTION\"]).to_pandas()\n",
    "\n",
    "f1_tree = f1_score(test_preds_tree.IS_FRAUD, test_preds_tree.FRAUD_PREDICTION)\n",
    "precision_tree = precision_score(test_preds_tree.IS_FRAUD, test_preds_tree.FRAUD_PREDICTION)\n",
    "recall_tree = recall_score(test_preds_tree.IS_FRAUD, test_preds_tree.FRAUD_PREDICTION)\n",
    "\n",
    "print(f'GB Tree: \\n f1: {f1_tree} \\n precision {precision_tree} \\n recall: {recall_tree}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66acb506-7931-4e01-9df5-af6b1df25111",
   "metadata": {
    "collapsed": false,
    "name": "sklearn_coding"
   },
   "source": [
    "### Sklearn code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fbb748-413b-4e8a-bdcc-ed4fedb09c17",
   "metadata": {
    "language": "python",
    "name": "sklearn_library"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Create the Model Registry and register your initial model\n",
    "from snowflake.ml.registry import Registry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15e5c83-b0e9-40a8-93dc-540468d54d99",
   "metadata": {
    "language": "python",
    "name": "sklearn_train_test"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ds_sp_pandas = ds_sp.to_pandas()\n",
    "\n",
    "# Assuming 'ds_sp' is a valid pandas DataFrame\n",
    "X = ds_sp_pandas[['DEVICE_TYPE', 'MERCHANT_CATEGORY', 'TRANSACTION_DAY', \n",
    "                  'TRANSACTION_AMOUNT', 'SENTIMENT_SCORE']]\n",
    "y = ds_sp_pandas['IS_FRAUD']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Optional: Display shapes to verify the split\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720010c4-fd6c-4e0d-b61f-01c3b0e40920",
   "metadata": {
    "language": "python",
    "name": "skllearn_pipeline_modelling"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Define categorical and numerical columns\n",
    "categorical_features = ['DEVICE_TYPE', 'MERCHANT_CATEGORY', 'TRANSACTION_DAY']\n",
    "numerical_features = ['TRANSACTION_AMOUNT', 'SENTIMENT_SCORE']\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ]), numerical_features),\n",
    "        ('cat', Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ]), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"XGBoost\": Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "    ]),\n",
    "    \"RandomForest\": Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier())\n",
    "    ]),\n",
    "    \"LogisticRegression\": Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression())\n",
    "    ])\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name} model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    score = accuracy_score(y_test, preds)\n",
    "    print(f\"{name} Accuracy: {score:.4f}\")\n",
    "\n",
    "    if score > best_score:\n",
    "        best_model = model\n",
    "        best_score = score\n",
    "\n",
    "print(f\"Best model: {type(best_model.named_steps['classifier']).__name__} with accuracy {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd079cea-1aa3-412f-b3a2-8e3712efba07",
   "metadata": {
    "collapsed": false,
    "name": "model_registry_md_snowml"
   },
   "source": [
    "# Logging the model to Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b682614-1012-4549-9e14-6ba5ddf65e7f",
   "metadata": {
    "language": "python",
    "name": "define_model_registry"
   },
   "outputs": [],
   "source": [
    "from snowflake.ml._internal.utils import identifier\n",
    "#Create a snowflake model registry object \n",
    "db = identifier._get_unescaped_name(session.get_current_database())\n",
    "schema = identifier._get_unescaped_name(session.get_current_schema())\n",
    "\n",
    "model_registry = Registry(session=session, \n",
    "                    database_name=session.get_current_database(), \n",
    "                    schema_name=session.get_current_schema(),\n",
    "                    options={\"enable_monitoring\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0467afc0-7926-4958-a748-58a3cfa86c2a",
   "metadata": {
    "language": "python",
    "name": "register_model_version_sklearn"
   },
   "outputs": [],
   "source": [
    "#Deploy the sklearn model to the model registry\n",
    "# Define model name\n",
    "model_name = \"FRAUD_ANALYSIS\"\n",
    "version_name = 'V5'\n",
    "\n",
    "try:\n",
    "    mv = model_registry.get_model(model_name).version(version_name)\n",
    "    print(\"Found existing model version!\")\n",
    "except:\n",
    "    print(\"Logging new model version...\")\n",
    "    mv  = model_registry.log_model(\n",
    "        model_name=model_name,\n",
    "        model=best_model, \n",
    "        version_name=version_name,\n",
    "        sample_input_data=X_train,\n",
    "        comment = \"sklearn model build\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e7dec9-6f5e-49c9-a72b-00c0280eb4d0",
   "metadata": {
    "language": "python",
    "name": "register_model_version_xgb"
   },
   "outputs": [],
   "source": [
    "#Deploy the tree booster model to the model registry\n",
    "# Define model name\n",
    "model_name = \"FRAUD_ANALYSIS_XGB\"\n",
    "tree_version_name = 'V3'\n",
    "\n",
    "try:\n",
    "    mv_tree = model_registry.get_model(model_name).version(tree_version_name)\n",
    "    print(\"Found existing model version!\")\n",
    "except:\n",
    "    print(\"Logging new model version...\")\n",
    "    mv_tree = model_registry.log_model(\n",
    "        model_name=model_name,\n",
    "        model=snow_xgb_tree, \n",
    "        version_name=tree_version_name,\n",
    "        comment = \"snow ml model built off feature store using tree booster\",\n",
    "    )\n",
    "    mv_tree.set_metric(metric_name=\"F1_score\", value=f1_tree)\n",
    "    mv_tree.set_metric(metric_name=\"Precision_score\", value=precision_tree)\n",
    "    mv_tree.set_metric(metric_name=\"Recall_score\", value=recall_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631a4bb-3fc1-4ca8-b0b0-a16192e4089a",
   "metadata": {
    "language": "python",
    "name": "model_registry_show"
   },
   "outputs": [],
   "source": [
    "model_registry.show_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d1311-cebe-446f-acfd-44e894e96afd",
   "metadata": {
    "language": "python",
    "name": "model_registry_delete"
   },
   "outputs": [],
   "source": [
    "## model_registry.delete_model(\"FRAUD_ANALYSIS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc2320-6dab-402e-810d-1ac09ea60d62",
   "metadata": {
    "language": "python",
    "name": "show_model_versions"
   },
   "outputs": [],
   "source": [
    "model_registry.get_model(\"FRAUD_ANALYSIS\").show_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d0aba-fa5d-48e3-a6df-7645cac1032d",
   "metadata": {
    "language": "python",
    "name": "model_retrieve_sklearn"
   },
   "outputs": [],
   "source": [
    "### retrieving sklearn - frad model - version 1 \n",
    "reg_model = model_registry.get_model(\"FRAUD_ANALYSIS\").version(\"v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80117bc9-164d-446b-856a-2b14cf828d10",
   "metadata": {
    "language": "python",
    "name": "model_retrieve_tree"
   },
   "outputs": [],
   "source": [
    "### retrieving sklearn - frad model - version 1 \n",
    "reg_model_tree = model_registry.get_model(\"FRAUD_ANALYSIS_XGB\").version(\"v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c360d13-c6f0-4a8b-9772-80b0abd64398",
   "metadata": {
    "language": "python",
    "name": "required_model"
   },
   "outputs": [],
   "source": [
    "reg_model_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d25e0-cf07-4fef-a74b-6f85e3b56a50",
   "metadata": {
    "language": "python",
    "name": "print_model_version_and_metrics"
   },
   "outputs": [],
   "source": [
    "print(mv_tree)\n",
    "print(mv_tree.show_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d21d856-83ac-4a48-8770-009c1677b769",
   "metadata": {
    "collapsed": false,
    "name": "intro_batch_inferencing"
   },
   "source": [
    "# Batch Inferencing on warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45871620-2429-451a-87f4-f6a2168f4ab1",
   "metadata": {
    "collapsed": false,
    "name": "batch_inferencing_tree_model"
   },
   "source": [
    "#### Two model what we have trained Sklearn & snowml lets predict using both the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54de8e92-d01d-4d65-88ed-bad264e36856",
   "metadata": {
    "language": "python",
    "name": "predict_from_snowml_model"
   },
   "outputs": [],
   "source": [
    "reg_preds_tree = mv_tree.run(test, function_name = \"predict\")\n",
    "reg_preds_tree.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e84a95-06c1-4b62-8fd1-be6d0ba0ae0e",
   "metadata": {
    "language": "python",
    "name": "predict_from_probability_snowml_model"
   },
   "outputs": [],
   "source": [
    "reg_probs_tree_prob = mv_tree.run(test, function_name=\"predict_proba\") \n",
    "reg_probs_tree_prob.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3972da8d-3686-476f-83c3-41118c9a1de3",
   "metadata": {
    "language": "python",
    "name": "prediction_on_complete_dataset"
   },
   "outputs": [],
   "source": [
    "### Extracting customer id, transaction Id, and prediction \n",
    "\n",
    "reg_probs_tree_prob_complete = mv_tree.run(ds_sp_ohe, \n",
    "                            function_name=\"predict_proba\").select(\"CUSTOMER_ID\", \n",
    "                        \"TRANSACTION_ID\", \"PREDICT_PROBA_0\", \"PREDICT_PROBA_1\" ) \n",
    "\n",
    "reg_probs_tree_prob_complete.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6b520-212a-4183-b4e0-62c02fdcc08a",
   "metadata": {
    "language": "python",
    "name": "Master_dataset"
   },
   "outputs": [],
   "source": [
    "### Earlier data set before split \n",
    "ds_sp.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac8a5a-9538-4ceb-ad4c-1c2773392bc1",
   "metadata": {
    "language": "python",
    "name": "master_data_creation_with_prediction"
   },
   "outputs": [],
   "source": [
    "# left join master dataset\n",
    "joined_df = ds_sp.join(\n",
    "    reg_probs_tree_prob_complete,\n",
    "    (ds_sp[\"customer_id\"] == reg_probs_tree_prob_complete[\"customer_id\"]) & (ds_sp[\"transaction_id\"] == reg_probs_tree_prob_complete[\"transaction_id\"]),\n",
    "    join_type=\"left\",\n",
    ")\n",
    "\n",
    "# Prioritize columns from ds_sp and select all columns without renaming\n",
    "selected_columns = [ds_sp[col] for col in ds_sp.columns] + \\\n",
    "                   [reg_probs_tree_prob_complete[\"predict_proba_0\"], reg_probs_tree_prob_complete[\"predict_proba_1\"]]\n",
    "\n",
    "# Use select to build dataframe with those specified columns and save the table\n",
    "final_df = joined_df.select(*selected_columns)\n",
    "\n",
    "final_df.show(2)\n",
    "# Save the dataframe\n",
    "#final_df.write.mode(\"overwrite\").save_as_table(\"FT_Prediction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98270841-d5c9-4104-9673-b95d0c8c3a5f",
   "metadata": {
    "language": "sql",
    "name": "final_dataset"
   },
   "outputs": [],
   "source": [
    "select * from FT_Prediction limit 5; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8ae24c-bcaf-4c9e-982b-4fca8ab3b923",
   "metadata": {
    "collapsed": false,
    "name": "Prediction_using_sklearn"
   },
   "source": [
    "## Batch prediction using sk learn model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93efb51-814f-4181-9c4e-0fa892828bfe",
   "metadata": {
    "language": "python",
    "name": "predict_prob_from_sklearn_model"
   },
   "outputs": [],
   "source": [
    "reg_probs_sk = mv.run(X_test, function_name=\"predict_proba\") \n",
    "reg_preds_sk = mv.run(X_test, function_name = \"predict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cbd617-0694-425c-be51-a46c63fbcd93",
   "metadata": {
    "language": "python",
    "name": "print_prediction_sklearn"
   },
   "outputs": [],
   "source": [
    "print(reg_probs_sk.head(2))\n",
    "print(reg_preds_sk.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc22a252-fe1d-4992-a3e1-9670dc87f375",
   "metadata": {
    "collapsed": false,
    "name": "feature_Importance"
   },
   "source": [
    "### Feature importance and score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ccdaa7-529a-4077-821f-b0b29055b8ce",
   "metadata": {
    "language": "python",
    "name": "feature_importance"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get feature names from preprocessor\n",
    "feature_names = (models[\"RandomForest\"]\n",
    "                 .named_steps[\"preprocessor\"]\n",
    "                 .transformers_[0][2] +  # Numerical features\n",
    "                 list(models[\"RandomForest\"].named_steps[\"preprocessor\"]\n",
    "                      .transformers_[1][1]\n",
    "                      .named_steps[\"encoder\"]\n",
    "                      .get_feature_names_out(categorical_features)))  # Encoded categorical features\n",
    "\n",
    "# Get feature importance from the RandomForestClassifier\n",
    "importances = best_model.named_steps[\"classifier\"].feature_importances_\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "feature_importance_df = pd.DataFrame(\n",
    "    {\"Feature\": feature_names, \"Importance\": importances}\n",
    ").sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Print feature importances\n",
    "print(feature_importance_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1403f3-069d-4976-9e80-2ab386136a72",
   "metadata": {
    "language": "python",
    "name": "feature_importance_graph"
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(feature_importance_df[\"Feature\"], feature_importance_df[\"Importance\"])\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Feature Importance for RandomForestClassifier Model\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e3371-a827-4f48-a125-edc883573b6d",
   "metadata": {
    "collapsed": false,
    "name": "model_explanability"
   },
   "source": [
    "# Model explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d66755b-5242-439c-85d9-b7169d45c5e8",
   "metadata": {
    "language": "python",
    "name": "explanability"
   },
   "outputs": [],
   "source": [
    "shap_vals = mv_tree.run(test.sample(n=1000), function_name=\"explain\")\n",
    "shap_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fa9ff9-e267-42e9-8bf5-c85c55cf08c6",
   "metadata": {
    "language": "python",
    "name": "explanability_pandas"
   },
   "outputs": [],
   "source": [
    "shap_pd = shap_vals.to_pandas()\n",
    "shap_pd.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2153f740-5205-4229-8344-2c54886b7bde",
   "metadata": {
    "language": "python",
    "name": "pip_install_shap"
   },
   "outputs": [],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d5b5a-9fb0-4a27-86e9-34107c2d8756",
   "metadata": {
    "collapsed": false,
    "name": "real_time_inference"
   },
   "source": [
    "## Real time inferencing deployment on Snowpark Container Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc685075-96b9-4b92-9850-770f3404c7a3",
   "metadata": {
    "language": "python",
    "name": "definning_varibales"
   },
   "outputs": [],
   "source": [
    "image_repo_name = \"FRAUD_PREDICTION_REPO\"\n",
    "num_spcs_nodes = '3'\n",
    "spcs_instance_family = 'CPU_X64_L'\n",
    "service_name = 'FRAUD_PREDICTION_SERVICE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79da0d5-442d-4959-9009-222e3d83dd9b",
   "metadata": {
    "language": "python",
    "name": "calling_universal_variable"
   },
   "outputs": [],
   "source": [
    "current_database = session.get_current_database().replace('\"', '')\n",
    "current_schema = session.get_current_schema().replace('\"', '')\n",
    "extended_image_repo_name = f\"{current_database}.{current_schema}.{image_repo_name}\"\n",
    "extended_service_name = f'{current_database}.{current_schema}.{service_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f2c12-c9b7-4c76-a865-af1df6ed6e99",
   "metadata": {
    "language": "python",
    "name": "code_exp"
   },
   "outputs": [],
   "source": [
    "# # Make predictions\n",
    "# results_df = X_test.copy()\n",
    "# # Get predictions and probabilities\n",
    "# y_pred = mv.run(results_df, function_name=\"predict\")\n",
    "# y_pred_proba = mv.run(results_df, function_name=\"predict_proba\")\n",
    "# results_df['PREDICTION'] = y_pred\n",
    "# results_df['PREDICTION_PROBABILITY'] = y_pred_proba.iloc[:, 1]  \n",
    "# print(results_df[['PREDICTION', 'PREDICTION_PROBABILITY']].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eab0fb-f8d7-4662-8ac1-509e899efc0e",
   "metadata": {
    "collapsed": false,
    "name": "Model_observability"
   },
   "source": [
    "#### do model drift \n",
    "#### observability \n",
    "### Explanability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88eee90-9b03-4320-85cd-5ad37e7c9eb3",
   "metadata": {
    "collapsed": false,
    "name": "model_drift"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7069492f-3f4e-4536-940c-450bff967f0a",
   "metadata": {
    "collapsed": false,
    "name": "Monitor_Model_observability"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ca6fa-7d69-4fa9-b2d1-daeeab9fbf7a",
   "metadata": {
    "language": "sql",
    "name": "model_monitor_code"
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM TABLE(MODEL_MONITOR_DRIFT_METRIC (\n",
    "    'FRAUD_ANALYSIS',            -- The name of your model to monitor\n",
    "    'accuracy_score',            -- The drift metric (e.g., accuracy_score, f1_score)\n",
    "    'TRANSACTION_AMOUNT',        -- The feature you want to monitor for drift (e.g., 'TRANSACTION_AMOUNT')\n",
    "    'DAY',                       -- Granularity (e.g., 'DAY', 'HOUR', 'MONTH')\n",
    "    '2025-01-01',                -- Start time (in 'YYYY-MM-DD' format)\n",
    "    '2025-03-01'                 -- End time (in 'YYYY-MM-DD' format)\n",
    "));\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da14fc2a-f2dd-4f01-98c2-b4a3d31b735f",
   "metadata": {
    "language": "sql",
    "name": "model_task_code"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TASK MY_MODEL_TASK\n",
    "    WAREHOUSE = 'FRAUD_WH'  -- Specify your warehouse\n",
    "    SCHEDULE = 'USING CRON 0 0 * * * UTC'  -- Schedule it daily (adjust cron as needed)\n",
    "AS\n",
    "    CALL MONITOR_MODEL_PERFORMANCE();  -- Replace with your actual monitoring logic or stored procedure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac66490-082d-4a9e-b4e7-d466c5aa4377",
   "metadata": {
    "language": "sql",
    "name": "create_model_monitor"
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE MODEL MONITOR FRAUD_ANALYSIS_MONITOR\n",
    "    WITH\n",
    "        MODEL = 'FRAUD_ANALYSIS'\n",
    "        VERSION = 'V3'\n",
    "        FUNCTION = 'PREDICT'\n",
    "        SOURCE = 'TRANSACTIONS'\n",
    "        WAREHOUSE = 'MY_WAREHOUSE'\n",
    "        REFRESH_INTERVAL = '1 DAY'\n",
    "        AGGREGATION_WINDOW = '30 DAY'\n",
    "        TIMESTAMP_COLUMN = 'TRANSACTION_TIME'\n",
    "        BASELINE = 'initial_model_baseline'\n",
    "        ID_COLUMNS = ('CUSTOMER_ID')\n",
    "        PREDICTION_CLASS_COLUMNS = ('PREDICTED_IS_FRAUD')\n",
    "        ACTUAL_CLASS_COLUMNS = ('IS_FRAUD');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5e242-e7a1-4188-9517-a0b35d3bf077",
   "metadata": {
    "language": "sql",
    "name": "App_code"
   },
   "outputs": [],
   "source": [
    "CREATE STAGE ml_model;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "prash.medirattaa@snowflake.com",
   "authorId": "127400340872",
   "authorName": "PRASHMED",
   "lastEditTime": 1741627573065,
   "notebookId": "zs7i7oenwbhov3j2jnba",
   "sessionId": "fde869ca-1ce0-4a1f-8763-2007487308be"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
