{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "5zrmlzi4ycd3y3bwkjnu",
   "authorId": "127400340872",
   "authorName": "PRASHMED",
   "authorEmail": "prash.medirattaa@snowflake.com",
   "sessionId": "29d657de-b59f-4467-bcdc-dac4730a6f00",
   "lastEditTime": 1741453236212
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de2aef0-1e86-4b22-adf3-97a1c4382217",
   "metadata": {
    "name": "End_to_End_AI_Project",
    "collapsed": false
   },
   "source": "### End to End ML In Snowflake\n\n1. Data Generation \n2. EDA\n3.Feature Engineering \n4.Use Feature Store to track engineered features\n    - Store feature defintions in feature store for reproducible computation of ML features\n2. Train two SnowML Models\n    - Xgboost with tree booster\n    - Xgboost with linear booster\n3. Register both models in Snowflake model registry\n    - Explore model registry capabilities such as metadata tracking, inference, and explainability\n4. Set up Model Monitor to track 1 year of predicted and actual loan repayments\n    - Compute performance metrics such a F1, Precision, Recall\n    - Inspect model drift (i.e. how much has the average predicted repayment rate changed day-to-day)\n    - Compare models side-by-side to understand which model should be used in production\n    - Identify and understand data issues\n5. Track data and model lineage throughout\n    - View and understand\n        * The origin of the data used for computed features\n        * The data used for model training\n        * The available model versions being monitored\n\n#### Flow of the presentation \n\n- Data Generation, Fraud detection, logs as well (cortex for summarizing )\n- EDA \n- Feature engineering \n- Use Feature Store to track engineered features\n- Train & evaluate models (XGBoost, RF, LR)\n- Model deploymeent & monitoring - Low latency less < 1 ,s ( SPCS & Warehouse )  ####clarify\n- App \n- cortex analyst to find answers \n"
  },
  {
   "cell_type": "code",
   "id": "ea0c876b-d5f9-4c61-855d-111e111ed26d",
   "metadata": {
    "language": "python",
    "name": "Importing_libraries"
   },
   "outputs": [],
   "source": "# Standard Python Libraries\nimport sys\nimport json\nimport warnings\nfrom datetime import timedelta\n\n# Data Manipulation and Analysis\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\n\n\n# Snowpark Core\nfrom snowflake.snowpark import Session, DataFrame\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.version import VERSION\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.exceptions import SnowparkSessionException\nfrom snowflake.snowpark.functions import sproc, col, dayname\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark.window import Window\n\n# Snowpark ML\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error\nfrom snowflake.ml.registry import Registry\n\n# Snowflake Feature Store\nfrom snowflake.ml.feature_store import (\n    FeatureStore, FeatureView, Entity, CreationMode, setup_feature_store\n)\n\n# Snowflake Task API\nfrom snowflake.core import Root\nfrom snowflake.core.database import Database\nfrom snowflake.core.schema import Schema\nfrom snowflake.core.warehouse import Warehouse\nfrom snowflake.core.task import StoredProcedureCall\nfrom snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation\nfrom snowflake.core._common import CreateMode\n\n# Streamlit\nimport streamlit as st\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Create the Model Registry and register your initial model\nfrom snowflake.ml.registry import Registry\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5cfd9091-b782-450c-bbf2-b5a129b115af",
   "metadata": {
    "language": "python",
    "name": "session_start",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Create Snowflake Session object\nsession = get_active_session()\nsession.sql_simplifier_enabled = True\n\nsnowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\nsnowpark_version = VERSION\n\n# Current Environment Details\nprint('\\nConnection Established with the following parameters:')\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3a62b34-28b6-429b-90c8-ffc9e6f6a2ab",
   "metadata": {
    "name": "Data_Generation",
    "collapsed": false
   },
   "source": "Data Generation Script TBD"
  },
  {
   "cell_type": "markdown",
   "id": "ffcd2433-a352-47fb-879e-e8a9cfcf4535",
   "metadata": {
    "name": "EDA",
    "collapsed": false
   },
   "source": "- Do some basics EDA "
  },
  {
   "cell_type": "code",
   "id": "80563a67-0bba-45ef-8e59-fa9deb7e71fc",
   "metadata": {
    "language": "sql",
    "name": "cell24",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from transactions limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84f93abc-053b-417b-9317-7c3db63e9776",
   "metadata": {
    "language": "sql",
    "name": "cell29",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from customer_complaints limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2164b6c3-ce3f-45e1-97af-40f683d11c3d",
   "metadata": {
    "language": "sql",
    "name": "cell7",
    "collapsed": false
   },
   "outputs": [],
   "source": "--drop table fraud_analysis;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30f88cbc-8753-4871-b791-5520f8c36ed2",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "Merge two datasets"
  },
  {
   "cell_type": "code",
   "id": "a319d9c5-e621-4e8d-87bc-ba72f7203638",
   "metadata": {
    "language": "sql",
    "name": "Merge_two_datasets",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE fraud_analysis AS\nSELECT \n    t.transaction_id, \n    t.customer_id, \n    t.transaction_amount, \n    t.is_fraud, \n    t.merchant_category,\n    t.device_type,\n    t.location,\n    t.transaction_time,\n    c.complaint_text, \n    c.keywords,\n    c.complaint_time\nFROM transactions t\nLEFT JOIN customer_complaints c\nON t.customer_id = c.customer_id,",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1043d068-7dee-49a2-9f7f-194d901858d2",
   "metadata": {
    "language": "sql",
    "name": "cell9",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from fraud_analysis limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fa094c4b-0d1d-486d-8d38-0e727b9856fc",
   "metadata": {
    "language": "python",
    "name": "cell12",
    "collapsed": false
   },
   "outputs": [],
   "source": "# print(fraud_analysis.head())\n# print(fraud_analysis.columns)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "705ab123-c621-4b79-a650-517f36ec8100",
   "metadata": {
    "language": "sql",
    "name": "cell51"
   },
   "outputs": [],
   "source": " --drop table  fraud_analysis;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9bc5a4ad-a867-4f58-b003-e3f38ad2fdc8",
   "metadata": {
    "name": "Adding_column_FE",
    "collapsed": false
   },
   "source": "- lets do feature engineering "
  },
  {
   "cell_type": "code",
   "id": "8a37c879-ba74-4923-aec7-244e23586b7e",
   "metadata": {
    "language": "sql",
    "name": "Adding_column",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- ALTER TABLE fraud_analysis \n-- ADD COLUMN computed_sentiment STRING;\n-- UPDATE fraud_analysis \n-- SET computed_sentiment = SNOWFLAKE.CORTEX.SENTIMENT(complaint_text);",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1af39890-82d2-42a1-8753-553cb5753f99",
   "metadata": {
    "language": "sql",
    "name": "cell31",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- SELECT complaint_text, computed_sentiment \n-- FROM fraud_analysis \n-- LIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0d005a7-b1d6-4910-b9fa-aa22e3d2410e",
   "metadata": {
    "name": "Feature_Store",
    "collapsed": false
   },
   "source": "### Create features with Feature Store"
  },
  {
   "cell_type": "markdown",
   "id": "8346a407-0b50-4183-9b88-0760c88eaae9",
   "metadata": {
    "name": "cell25",
    "collapsed": false
   },
   "source": "Initialize Feature Store\nLet's first create a feature store client. With CREATE_IF_NOT_EXIST mode, it will try to create a new Feature Store schema and all necessary feature store metadata if it doesn't exist already. It is required for the first time to set up a Feature Store. Afterwards, you can use FAIL_IF_NOT_EXIST mode to connect to an existing Feature Store.\n\nNote that the database being used must already exist. Feature Store will NOT try to create the database even in CREATE_IF_NOT_EXIST mode."
  },
  {
   "cell_type": "markdown",
   "id": "57fc63d4-ce89-49c3-97d7-7230c7a8f143",
   "metadata": {
    "name": "cell45",
    "collapsed": false
   },
   "source": "Generate cumulative behavioral metrics for users based on their transaction data, such as cumulative clicks and cumulative logins per hour. It involves the use of window functions and joins to combine and transform data from the CREDITCARD_TRANSACTIONS table."
  },
  {
   "cell_type": "code",
   "id": "e21befc8-4fc6-4738-92de-1897bd381739",
   "metadata": {
    "language": "python",
    "name": "Generate_Transaction_Features"
   },
   "outputs": [],
   "source": "feature_df = session.sql(\"\"\"\n  SELECT \n    customer_id,\n    AVG(transaction_amount) AS avg_transaction_amount,\n    COUNT(*) AS transaction_count,\n    MAX(transaction_time) AS last_transaction_time,\n    SUM(CASE WHEN is_fraud = TRUE THEN 1 ELSE 0 END) AS fraud_count,\n    MODE(merchant_category) AS most_common_merchant_category,\n    MODE(device_type) AS most_common_device_type,\n    MODE(location) AS most_common_location,\n    AVG(SNOWFLAKE.CORTEX.SENTIMENT(complaint_text)) AS avg_sentiment_score,\n    COUNT(CASE WHEN SNOWFLAKE.CORTEX.SENTIMENT(complaint_text) > 0 THEN 1 END) AS positive_sentiment_count,\n    COUNT(CASE WHEN SNOWFLAKE.CORTEX.SENTIMENT(complaint_text) < 0 THEN 1 END) AS negative_sentiment_count,\n    MODE(DAYNAME(transaction_time)) AS most_common_transaction_day\n  FROM fraud_analysis\n  GROUP BY customer_id\n\"\"\")\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "54504166-1f42-4ce7-9992-24d3b29f8b21",
   "metadata": {
    "language": "python",
    "name": "Writing_features"
   },
   "outputs": [],
   "source": "# feature_df.write.mode('overwrite').save_as_table('feature_df')\n# feature_df=session.sql(\"select * from feature_df\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37e7ebbb-d4f3-4d31-bde4-7ca90b907706",
   "metadata": {
    "language": "python",
    "name": "features_list"
   },
   "outputs": [],
   "source": "# Get the first two rows\nfeature_df.limit(2).collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52be0c04-dcc2-4584-ad63-50f38c6fbf85",
   "metadata": {
    "language": "python",
    "name": "feature_df_explain"
   },
   "outputs": [],
   "source": "feature_df.explain()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37e0bba9-1133-4e9e-8a6e-aa6dc2394c05",
   "metadata": {
    "name": "customertrans_entity",
    "collapsed": false
   },
   "source": "Creating Entities\n\nAn entity is an abstraction over a set of primary keys used for looking up feature data. An Entity represents a real-world \"thing\" that has data associated with it. Below cell registers an entity for Customer and Transaction in Feature Store"
  },
  {
   "cell_type": "code",
   "id": "2dd50344-d2fb-4ac8-adf0-78a6c3201ba2",
   "metadata": {
    "language": "python",
    "name": "define_feature_store"
   },
   "outputs": [],
   "source": "fs = FeatureStore(\n    session=session, \n    database=session.get_current_database(), \n    name=session.get_current_schema(), \n    default_warehouse=session.get_current_warehouse(),\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3147672e-dd22-4a8b-8818-8bc1f4f3247a",
   "metadata": {
    "language": "python",
    "name": "Feature_Entity"
   },
   "outputs": [],
   "source": "from snowflake.ml.feature_store import Entity\n\ncustomer_entity = Entity(\n    name=\"CUSTOMER\",\n    join_keys=[\"customer_id\"],\n    desc=\"Primary Key for Customer\"\n)\nfs.register_entity(customer_entity)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc5c788c-d46f-49e8-aebb-a2a78794465f",
   "metadata": {
    "language": "python",
    "name": "List_entities"
   },
   "outputs": [],
   "source": "fs.list_entities()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e19f2b0d-4fc2-41a4-959f-a0d549e0d413",
   "metadata": {
    "name": "Feature_Views",
    "collapsed": false
   },
   "source": "# Using Feature Views\n\nA feature view is a group of logically-related features that are refreshed on the same schedule. The FeatureView constructor accepts a Snowpark DataFrame that contains the feature generation logic. The provided DataFrame must contain the join_keys columns specified in the entities associated with the feature view. In this example we are using time-series data, so we will also specify the timestamp column name.\n\nBelow cell creates a feature view for the customer features\n"
  },
  {
   "cell_type": "code",
   "id": "fbdcc3b9-b7a0-4820-affc-5e41a6339d8c",
   "metadata": {
    "language": "python",
    "name": "load_or_register_entity"
   },
   "outputs": [],
   "source": "fraud_feature_view = FeatureView(\n    name=\"FRAUD_FEATURES\",\n    feature_df=feature_df,\n    entities=[customer_entity],\n    desc=\"Features derived from customer transactions for fraud detection\",\n    tags={\"domain\": \"fraud_detection\", \"update_frequency\": \"daily\"},\n    overwrite=True,\n    refresh_freq=\"1 day\"\n)\n# Register the feature view with the feature store\nregistered_fv=fs.register_feature_view(fraud_feature_view,version=\"V1\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b7067c5-5048-4a55-a462-31da4d337a10",
   "metadata": {
    "language": "python",
    "name": "registered_features"
   },
   "outputs": [],
   "source": "registered_fv",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5610bdf0-b63c-401d-9898-492c4e60966e",
   "metadata": {
    "name": "cell52",
    "collapsed": false
   },
   "source": "#### More code to ref later "
  },
  {
   "cell_type": "code",
   "id": "73ab6613-05c1-465d-8fd2-7f7dde1037f6",
   "metadata": {
    "language": "python",
    "name": "code_dump",
    "collapsed": false
   },
   "outputs": [],
   "source": "# # from snowflake.snowpark.functions import call_udf\n\n# # feature_eng_dict = dict()\n\n# # # New features\n# # feature_eng_dict[\"SENTIMENT_SCORE\"] = call_udf(\"SNOWFLAKE.CORTEX.SENTIMENT\", col(\"complaint_text\"))\n# # feature_eng_dict[\"TRANSACTION_DAY\"] = dayname(col(\"transaction_time\"))\n\n# # # Apply all features to the DataFrame\n# # df = df.with_columns(feature_eng_dict.keys(), feature_eng_dict.values())\n\n# # df.show(2)\n\n# # feature_df = df.select(\n# #     \"transaction_amount\", \n# #     \"merchant_category\", \n# #     *list(feature_eng_dict.keys())\n# # )\n# # feature_df.show(5)\n\n# # print(df.explain())\n# # #Create a dataframe with just the ID, timestamp, and engineered features. We will use this to define our feature view\n# # feature_df = df.select([[\"transaction_amount\",\"merchant_category\"]]+list(feature_eng_dict.keys()))\n# # feature_df.show(5)\n# # ds = fs.generate_dataset(\n# #     name=\"FRAUD_DETECTION_DATASET_V1\",\n# #     spine_df=feature_df.drop(\"TRANSACTION_AMOUNT\", \"IS_FRAUD\", \"SENTIMENT_SCORE\", \"SENTIMENT_CATEGORY\"),\n# #     features=[fraud_feature_view],\n# #     spine_timestamp_col=\"TRANSACTION_TIME\",\n# #     spine_label_cols=[\"IS_FRAUD\"]\n# # )\n# # registered_fv = fs.register_feature_view(fraud_feature_view, version=\"v2\", overwrite=True)\n# # print(registered_fv.status)  # This should print FeatureViewStatus.ACTIVE\n# # features=[fraud_feature_view]\n# # features\n# #print(fraud_feature_view.name)\n\n# # fs = FeatureStore(\n# #     session=session, \n# #     database=session.get_current_database(), \n# #     name=session.get_current_schema(), \n# #     default_warehouse=session.get_current_warehouse(),\n# #     creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n# # )\n\n# # from snowflake.ml.feature_store import Entity\n\n# # customer_entity = Entity(\n# #     name=\"CUSTOMER\",\n# #     join_keys=[\"customer_id\"],\n# #     desc=\"Primary Key for Customer\"\n# # )\n# # fs.register_entity(customer_entity)\n# # Delete a specific version of a feature view\n# # fs.delete_feature_view(\n# #     feature_view=\"FRAUD_FEATURES\",  \n# #     version=\"v2\"                   \n# # )\n\n# X = fraud_analysis[['TRANSACTION_AMOUNT', 'DEVICE_TYPE','MERCHANT_CATEGORY','LOCATION']]\n# y = fraud_analysis['IS_FRAUD']\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n# #Use the Snowpark DataFrame .describe function. You need to need to visualize from a pandas DataFrame\n# fraud_analysis.describe()\n\n# # Get transactions dataset and get features from the feature store\n# def create_dataset(spine_df, name):\n#     train_dataset = fs.generate_dataset(\n#     name=name,\n#     spine_df=spine_df,\n#     features=[customer_fv]\n#     )\n#     df = train_dataset.read.to_snowpark_dataframe()\n#     return df\n# # Split into train/validation/test\n\n\n# # Generate dataset for training\n# train_dataset = fs.generate_dataset(\n#     name=\"FRAUD_DETECTION_DATASET\",\n#     spine_df=spine_df,  # Now properly defined\n#     features=[customer_fv],\n#     spine_timestamp_col=\"transaction_time\",\n#     spine_label_cols=[\"is_fraud\"],\n#     version=\"v1\"\n# )\n # Ensure using sklearn-compatible XGBoost\n\n# models = {\n#     \"XGBoost\": XGBClassifier(),\n#     \"RandomForest\": RandomForestClassifier(),\n#     \"LogisticRegression\": LogisticRegression()\n# }\n\n# best_model = None\n# best_score = 0\n\n# for name, model in models.items():\n#     print(f\"Training {name} model...\")\n#     model.fit(X_train, y_train)  # Ensure fit method is correct\n#     preds = model.predict(X_test)\n#     score = accuracy_score(y_test, preds)\n#     print(f\"{name} Accuracy: {score:.4f}\")\n\n#     if score > best_score:\n#         best_model = model\n#         best_score = score\n\n# print(f\"Best model: {best_model} with accuracy {best_score:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "90323264-df3d-4679-ab8b-1c9ea3961120",
   "metadata": {
    "name": "cell3",
    "collapsed": false
   },
   "source": "This completes the setup for the Database objects and Feature Store Producer workflow. The data and the features which have been generated is available for the consumer with appropritate privileges. Time to head on to the next notebook!"
  },
  {
   "cell_type": "markdown",
   "id": "993a427d-078c-4144-848d-0253930e8a89",
   "metadata": {
    "name": "cell4",
    "collapsed": false
   },
   "source": "Generating Datasets for Training\nWe are now ready to generate our training set. We'll define a spine DataFrame to form the backbone of our generated dataset and pass it into FeatureStore.generate_dataset() along with our Feature Views.\n\nNOTE: The spine serves as a request template and specifies the entities, labels and timestamps (when applicable). The feature store then attaches feature values along the spine using an AS-OF join to efficiently combine and serve the relevant, point-in-time correct feature data."
  },
  {
   "cell_type": "code",
   "id": "76cb6e17-2899-4217-abf7-a0900c5eb1b4",
   "metadata": {
    "language": "sql",
    "name": "Dataset"
   },
   "outputs": [],
   "source": "select * from fraud_analysis limit 2 ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b10e67c1-0568-4d43-8543-b1fd3269f0ce",
   "metadata": {
    "language": "python",
    "name": "create_spine_df"
   },
   "outputs": [],
   "source": "session.sql(\"create or replace TABLE TRANSACTIONS_DATA (customer_id VARCHAR,TRANSACTION_ID VARCHAR(16777216),IS_FRAUD VARCHAR)\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ac28cff-5000-4f21-82f5-273a8dc21a85",
   "metadata": {
    "language": "python",
    "name": "TRANSACTIONS_DATA"
   },
   "outputs": [],
   "source": "session.sql(\"insert into TRANSACTIONS_DATA(customer_id,TRANSACTION_ID, IS_FRAUD) SELECT distinct customer_id,TRANSACTION_ID, IS_FRAUD FROM fraud_analysis\").collect()\nTRANSACTIONS_DATA_df = session.table(\"TRANSACTIONS_DATA\")\nTRANSACTIONS_DATA_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9e3f3ccd-a116-4afd-b48f-0578be0feac7",
   "metadata": {
    "name": "stats_using_describe",
    "collapsed": false
   },
   "source": "Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset’s distribution."
  },
  {
   "cell_type": "code",
   "id": "e208479b-81a8-4df8-b29a-f027700dca34",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "full_df = session.sql(\"SELECT * FROM fraud_analysis\")\nfull_df.describe()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a044ecf0-18fc-4108-a9ac-45a6a1890b51",
   "metadata": {
    "language": "python",
    "name": "list_columns"
   },
   "outputs": [],
   "source": "full_df.columns",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7960e56d-e991-461d-9f51-6af44a9a55bc",
   "metadata": {
    "name": "Chart_fraud_normal",
    "collapsed": false
   },
   "source": "Visualization of the fraud and normal data using a bar chart displayed in Streamlit. Shows the total number of distinct transactions for each fraud category."
  },
  {
   "cell_type": "code",
   "id": "2bbdb415-581c-40f8-92dc-39080c122553",
   "metadata": {
    "language": "python",
    "name": "visualization_stchart"
   },
   "outputs": [],
   "source": "# Load the dataset\ndataset=full_df.toPandas()\n# Group by 'IS_FRAUD' and count distinct TRANSACTION_ID\ndf= TRANSACTIONS_DATA_df.select( F.col(\"TRANSACTION_ID\"),F.col(\"IS_FRAUD\")).groupBy(F.col(\"IS_FRAUD\")) \\\n          .agg(F.count_distinct(F.col(\"TRANSACTION_ID\")).alias(\"TOTAL_FRAUD\")) \n\n\nst.bar_chart(df,x=\"IS_FRAUD\",y=\"TOTAL_FRAUD\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ebf0129a-2a3a-44b2-9e77-24d70607dda2",
   "metadata": {
    "name": "FS_Init",
    "collapsed": false
   },
   "source": "## Feature Store\nThe feature store contains feature views for customers and transactions. Model features will be accessed from the feature store.\n\n**Snowflake Feature:** Feature Store (PrPr) - Easily find features that work with your data"
  },
  {
   "cell_type": "code",
   "id": "33c73015-1c21-4359-ad95-e0ae9b35da0e",
   "metadata": {
    "language": "python",
    "name": "cell34"
   },
   "outputs": [],
   "source": "customer_fv : FeatureView = fs.get_feature_view(\n    name='FRAUD_FEATURES',\n    version='V1'\n)\nprint(customer_fv)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9f526a28-842e-4602-a06e-ba713328dad2",
   "metadata": {
    "name": "Train_Test_Split",
    "collapsed": false
   },
   "source": "Generate a training data set with the feature store’s generate_training_set method, which enriches a Snowpark DataFrame that contains the source data with the derived feature values"
  },
  {
   "cell_type": "code",
   "id": "00944879-2649-4467-9486-23b9aff96d0c",
   "metadata": {
    "language": "python",
    "name": "spine_dataset"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import functions as F\n\n# Example 1: Use raw data as spine\nspine_df = session.table(\"fraud_analysis\").select(\n    \"customer_id\", \n    \"transaction_time\", \n    \"is_fraud\"\n)\nspine_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cea27037-e92a-4569-b6df-ea60fca6668f",
   "metadata": {
    "language": "python",
    "name": "data_trainnning"
   },
   "outputs": [],
   "source": "# Generate dataset for training\ntrain_dataset = fs.generate_dataset(\n    name=\"FRAUD_DETECTION_DATASET\",\n    spine_df=spine_df,\n    features=[customer_fv],\n    spine_timestamp_col=\"transaction_time\",\n    spine_label_cols=[\"is_fraud\"]\n)\n\n# Convert to pandas DataFrame\nFraud_data = train_dataset.read.to_pandas()\nFraud_data",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a9d98382-db56-4c42-81e1-2355198fc73e",
   "metadata": {
    "name": "cell17",
    "collapsed": false
   },
   "source": "see how can i incorporate the version "
  },
  {
   "cell_type": "code",
   "id": "fb1f0398-b1cd-423a-8172-4e83af8207a7",
   "metadata": {
    "language": "python",
    "name": "columns_list"
   },
   "outputs": [],
   "source": "# Print all column names\nprint(Fraud_data.columns)\n\n# Expected output should include:\n# ['AVG_TRANSACTION_AMOUNT', 'MOST_COMMON_DEVICE_TYPE', ...]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d63ab54-61a0-4a16-a36b-c71a7314b371",
   "metadata": {
    "language": "python",
    "name": "split_train_test"
   },
   "outputs": [],
   "source": "X = Fraud_data[['AVG_TRANSACTION_AMOUNT', 'TRANSACTION_COUNT','MOST_COMMON_DEVICE_TYPE','MOST_COMMON_LOCATION','MOST_COMMON_MERCHANT_CATEGORY','MOST_COMMON_TRANSACTION_DAY','AVG_SENTIMENT_SCORE']]\ny = Fraud_data['IS_FRAUD']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "46dd43f5-47eb-4628-86f2-8e959a6494ea",
   "metadata": {
    "language": "python",
    "name": "Traim_head"
   },
   "outputs": [],
   "source": "X_train.head(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8ce02a71-9cd3-4184-9f51-07cd23099157",
   "metadata": {
    "language": "python",
    "name": "Y_head"
   },
   "outputs": [],
   "source": "y_train.head(2)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d70b526b-032b-4da0-bef3-6ee889d5dcf4",
   "metadata": {
    "name": "training_dataset",
    "collapsed": false
   },
   "source": "View the training dataset.\n\nThis contains the columns except for Ids. The Label is included here as this will be specified in the LABEL field during model training."
  },
  {
   "cell_type": "markdown",
   "id": "829ffe07-491f-4386-888a-8a6a2b5d101b",
   "metadata": {
    "name": "Training_the_model",
    "collapsed": false
   },
   "source": "# Training the model\n"
  },
  {
   "cell_type": "code",
   "id": "3057183d-c1ca-4e2c-b2ee-a5a1f0b8a0e9",
   "metadata": {
    "language": "python",
    "name": "libary"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Create the Model Registry and register your initial model\nfrom snowflake.ml.registry import Registry",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "633279b6-a4ff-4a7f-8db6-095582d361a8",
   "metadata": {
    "language": "python",
    "name": "preprocessing"
   },
   "outputs": [],
   "source": "# Define categorical and numerical columns\ncategorical_features = ['MOST_COMMON_DEVICE_TYPE', 'MOST_COMMON_LOCATION',\n                        'MOST_COMMON_MERCHANT_CATEGORY', 'MOST_COMMON_TRANSACTION_DAY']\nnumerical_features = ['AVG_TRANSACTION_AMOUNT', 'TRANSACTION_COUNT', 'AVG_SENTIMENT_SCORE']\n\n# Create preprocessor\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', SimpleImputer(strategy='median'), numerical_features),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n    ])\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f57cb84-216c-4da2-9c5f-78f3b03bf4ed",
   "metadata": {
    "language": "python",
    "name": "define_model"
   },
   "outputs": [],
   "source": "# Define models\nmodels = {\n    \"XGBoost\": Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n    ]),\n    \"RandomForest\": Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', RandomForestClassifier())\n    ]),\n    \"LogisticRegression\": Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', LogisticRegression())\n    ])\n}\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2fad6ef-187e-4894-a198-dfdf81352a24",
   "metadata": {
    "language": "python",
    "name": "best_model_identified"
   },
   "outputs": [],
   "source": "best_model = None\nbest_score = 0\n\nfor name, model in models.items():\n    print(f\"Training {name} model...\")\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = accuracy_score(y_test, preds)\n    print(f\"{name} Accuracy: {score:.4f}\")\n\n    if score > best_score:\n        best_model = model\n        best_score = score\n\nprint(f\"Best model: {type(best_model.named_steps['classifier']).__name__} with accuracy {best_score:.4f}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2c07cec-4a15-4bd9-ab55-6a84b5c471c6",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false
   },
   "outputs": [],
   "source": "import numpy as np\n\nprint(\"y_train distribution:\", np.bincount(y_train))\nprint(\"y_test distribution:\", np.bincount(y_test))\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3b7bdf7-a53d-4117-9dda-cf3f7fb84591",
   "metadata": {
    "name": "Model_registry",
    "collapsed": false
   },
   "source": "# Logging the model to Model Registry"
  },
  {
   "cell_type": "code",
   "id": "de4e348e-465d-4571-8401-13cecf206dd8",
   "metadata": {
    "language": "python",
    "name": "MRegistry"
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\n\n\nregistry = Registry(session=session, \n                    database_name=session.get_current_database(), \n                    schema_name=session.get_current_schema(),\n                    options={\"enable_monitoring\": True})\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a06dc8bc-559a-42d3-ab7f-eaf4e70cc387",
   "metadata": {
    "language": "python",
    "name": "register"
   },
   "outputs": [],
   "source": "model_name = \"Fraud_Classification_model\"\n\nmodel_registry=registry.log_model(\n    model=best_model,\n    model_name=model_name,\n    version_name=\"v3\",\n    sample_input_data=X_train,\n    comment=\"My model trained with feature views, dataset\"\n)\nmodel_registry",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53dce6a2-f6b4-4471-9237-370f1792e5eb",
   "metadata": {
    "language": "python",
    "name": "show_models"
   },
   "outputs": [],
   "source": "registry.show_models()\n##SHOW MODELS IN DATABASE FRAUD_DB;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f7aa9cd5-6a77-4289-aa41-e9cfd811f66e",
   "metadata": {
    "language": "python",
    "name": "delete_model",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "### registry.delete_model(\"FRAUD_CLASSIFICATION_0\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5674839-de2b-4bd8-8e2f-5a785fccf159",
   "metadata": {
    "language": "python",
    "name": "retrieve_model"
   },
   "outputs": [],
   "source": "reg_model = registry.get_model(\"FRAUD_CLASSIFICATION_MODEL\").version(\"v1\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "328e6e06-23a7-4190-b198-9b0ac564fc80",
   "metadata": {
    "language": "python",
    "name": "TEST_DATASET"
   },
   "outputs": [],
   "source": "X_test.head(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a262853-5a94-4dd7-85d9-1bc3a6e79417",
   "metadata": {
    "language": "python",
    "name": "prediction"
   },
   "outputs": [],
   "source": "remote_prediction = reg_model.run(X_test, function_name='predict')\nremote_prediction\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b2b2520-65d9-43a4-b129-6fc38a130bcb",
   "metadata": {
    "language": "python",
    "name": "cell14"
   },
   "outputs": [],
   "source": "print(model_registry)\nprint(model_registry.show_metrics())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0fcce34b-26be-4a6c-99f6-03dd200e1011",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "# model_registry.show_functions()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08112f38-a5f4-455c-8e2a-9b5646001b7d",
   "metadata": {
    "language": "python",
    "name": "cell8"
   },
   "outputs": [],
   "source": "from sklearn import datasets, ensemble\n\niris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\niris_X.columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\nclf = ensemble.RandomForestClassifier(random_state=42)\nclf.fit(iris_X, iris_y)\n\nmodel_ref = registry.log_model(\n    clf,\n    model_name=\"RandomForestClassifier\",\n    version_name=\"v1\",\n    sample_input_data=iris_X,\n    options={\n        \"method_options\": {\n            \"predict\": {\"case_sensitive\": False},\n            \"predict_proba\": {\"case_sensitive\": False},\n            \"predict_log_proba\": {\"case_sensitive\": False},\n        }\n    },\n)\nresult = model_ref.run(iris_X[-10:], function_name=\"predict_proba\")\nprint(result)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c277f518-e3e1-4a46-8fb2-21cade7c6c11",
   "metadata": {
    "language": "python",
    "name": "cell11"
   },
   "outputs": [],
   "source": "from sklearn import datasets, ensemble, pipeline, preprocessing\n\niris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\npipe = pipeline.Pipeline([\n    ('scaler', preprocessing.StandardScaler()),\n    ('classifier', ensemble.RandomForestClassifier(random_state=42)),\n])\npipe.fit(iris_X, iris_y)\nmodel_ref = registry.log_model(\n    pipe,\n    model_name=\"Pipeline\",\n    version_name=\"v1\",\n    sample_input_data=iris_X,\n    options={\n        \"method_options\": {\n            \"predict\": {\"case_sensitive\": True},\n            \"predict_proba\": {\"case_sensitive\": True},\n            \"predict_log_proba\": {\"case_sensitive\": True},\n        }\n    },\n)\nmodel_ref.run(iris_X[-10:], function_name='\"predict_proba\"')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7d0b4dc1-ec0c-45a6-8495-42bb434b9dea",
   "metadata": {
    "language": "python",
    "name": "cell10"
   },
   "outputs": [],
   "source": "iris_y",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f454041-5a01-415e-bb76-3e5b1b1b96e5",
   "metadata": {
    "language": "python",
    "name": "cell22"
   },
   "outputs": [],
   "source": "# We can see what the default model is when we have multiple versions with the same model name:\nregistry.get_model(model_name).default.version_name",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1aca57b-7597-45da-bac3-78b8d717afa9",
   "metadata": {
    "language": "python",
    "name": "cell26"
   },
   "outputs": [],
   "source": "# # Now we can use the default version model to perform inference.\n# model_ver = registry.get_model(model_name).version('V0')\n# result_sdf = model_ver.run(X_test, function_name=\"predict\")\n# result_sdf\n# model_ver",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e32db33c-5ee5-457f-9c4f-282fc8d1df61",
   "metadata": {
    "name": "othermodel_experiment",
    "collapsed": false
   },
   "source": "Experiment the other xgboost model "
  },
  {
   "cell_type": "code",
   "id": "3a0a7bd4-341d-404d-a489-0c41171c5333",
   "metadata": {
    "language": "python",
    "name": "cell23"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OneHotEncoder, MinMaxScaler\nfrom snowflake.ml.modeling.pipeline import Pipeline\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "448a5358-302e-4ff5-bcab-ea74d5b78697",
   "metadata": {
    "language": "python",
    "name": "cell21"
   },
   "outputs": [],
   "source": "\n# Define categorical and numerical columns\ncategorical_features = ['MOST_COMMON_DEVICE_TYPE', 'MOST_COMMON_LOCATION',\n                        'MOST_COMMON_MERCHANT_CATEGORY', 'MOST_COMMON_TRANSACTION_DAY']\nnumerical_features = ['AVG_TRANSACTION_AMOUNT', 'TRANSACTION_COUNT', 'AVG_SENTIMENT_SCORE']\n\n# Create preprocessor pipeline\npreprocessing_pipeline = Pipeline(\n    steps=[\n        ('num_imputer', SimpleImputer(\n            input_cols=numerical_features,\n            output_cols=numerical_features,\n            strategy='median'\n        )),\n        ('num_scaler', MinMaxScaler(\n            input_cols=numerical_features,\n            output_cols=[f\"{col}_scaled\" for col in numerical_features],\n            clip=True\n        )),\n        ('cat_encoder', OneHotEncoder(\n            input_cols=categorical_features,\n            output_cols=[f\"{col}_encoded\" for col in categorical_features],\n            handle_unknown='ignore'\n        ))\n    ]\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "18f3aab2-9ae1-406e-9663-ec65a7ca5182",
   "metadata": {
    "language": "python",
    "name": "cell35"
   },
   "outputs": [],
   "source": "import joblib\nPIPELINE_FILE = '/tmp/preprocessing_pipeline.joblib'\njoblib.dump(preprocessing_pipeline, PIPELINE_FILE) # We are just pickling it locally first",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee78705d-27a5-4272-bf9a-aac60b29a033",
   "metadata": {
    "language": "python",
    "name": "cell37"
   },
   "outputs": [],
   "source": "df = preprocessing_pipeline.fit(Fraud_data).transform(Fraud_data)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3d1e28dd-4e6f-41a1-8926-23d14677f91b",
   "metadata": {
    "language": "python",
    "name": "cell36"
   },
   "outputs": [],
   "source": "transformed_diamonds_df = preprocessing_pipeline.fit(diamonds_df).transform(diamonds_df)\ntransformed_diamonds_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "65dfad23-3940-4ec3-b8b9-fba3eb944941",
   "metadata": {
    "language": "python",
    "name": "cell27"
   },
   "outputs": [],
   "source": "# Fit and transform the data\ntransformed_fraud_df = preprocessing_pipeline.fit(Fraud_data).transform(Fraud_data)\n\ntransformed_fraud_df\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6ebd28a-6941-4b14-9bca-9afd8a283dc7",
   "metadata": {
    "language": "python",
    "name": "cell18"
   },
   "outputs": [],
   "source": "# # Generate dataset for training\n# train_dataset = fs.generate_dataset(\n#     name=\"FRAUD_DETECTION_DATASET\",\n#     spine_df=spine_df,\n#     features=[customer_fv],\n#     spine_timestamp_col=\"transaction_time\",\n#     spine_label_cols=[\"is_fraud\"]\n# )\n\n# # Convert to pandas DataFrame\n# Fraud_data = train_dataset.read.to_pandas()\n# Fraud_data",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f496133-5a9a-4272-b1c9-4329d69f8831",
   "metadata": {
    "language": "python",
    "name": "cell19"
   },
   "outputs": [],
   "source": "Fraud_data.head(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae611ccd-df76-455d-be8a-9db58a825999",
   "metadata": {
    "language": "python",
    "name": "xg_part1"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(Fraud_data, test_size=0.1, random_state=0)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "398c7715-1170-4285-8c9c-4c0ec0d30f10",
   "metadata": {
    "language": "python",
    "name": "cell20"
   },
   "outputs": [],
   "source": "# Run the train and test sets through the Pipeline object we defined earlier\ntrain_df = preprocessing_pipeline.fit(diamonds_train_df).transform(diamonds_train_df)\ntest_df = preprocessing_pipeline.transform(diamonds_test_df)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d36c6d1-c62f-435b-92fc-b14f4f0189d4",
   "metadata": {
    "language": "python",
    "name": "cell15"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  }
 ]
}