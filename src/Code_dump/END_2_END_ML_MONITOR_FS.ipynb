{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "epxjzyyhoa4rwpyu77v6",
   "authorId": "127400340872",
   "authorName": "PRASHMED",
   "authorEmail": "prash.medirattaa@snowflake.com",
   "sessionId": "1941cb25-85f7-4513-ac77-24006f5375c6",
   "lastEditTime": 1741735104332
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de2aef0-1e86-4b22-adf3-97a1c4382217",
   "metadata": {
    "name": "End_to_End_AI_Project",
    "collapsed": false
   },
   "source": "### End to End ML In Snowflake\n\n1. Data Generation \n2. EDA\n3.Feature Engineering \n4.Use Feature Store to track engineered features\n    - Store feature defintions in feature store for reproducible computation of ML features\n2. Train two SnowML Models and multiple sciktlearn model \n    - Xgboost with tree booster\n    - Xgboost with linear booster\n3. Register both models in Snowflake model registry\n    - Explore model registry capabilities such as metadata tracking, inference, and explainability\n4. Set up Model Monitor to track 1 year of predicted and actual loan repayments\n    - Compute performance metrics such a F1, Precision, Recall\n    - Inspect model drift (i.e. how much has the average predicted repayment rate changed day-to-day)\n    - Compare models side-by-side to understand which model should be used in production\n    - Identify and understand data issues\n5. Track data and model lineage throughout\n    - View and understand\n        * The origin of the data used for computed features\n        * The data used for model training\n        * The available model versions being monitored\n6. Create a App leveraging the prediction to find answers for business users \n\n\n"
  },
  {
   "cell_type": "code",
   "id": "6fea6e9c-2405-4d58-81a9-89a1ef647660",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "-- CREATE OR REPLACE NETWORK RULE allow_all_rule\n-- MODE = 'EGRESS'\n-- TYPE = 'HOST_PORT'\n-- VALUE_LIST = ('0.0.0.0:443','0.0.0.0:80');\n\n-- CREATE OR REPLACE EXTERNAL ACCESS INTEGRATION allow_all_integration\n-- ALLOWED_NETWORK_RULES = (allow_all_rule)\n-- ENABLED = true;\n\n-- GRANT USAGE ON INTEGRATION allow_all_integration TO ROLE PUBLIC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a89da9ed-a680-42bd-b608-2258ed33bc60",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "#!pip install shap",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ea0c876b-d5f9-4c61-855d-111e111ed26d",
   "metadata": {
    "language": "python",
    "name": "Importing_libraries"
   },
   "outputs": [],
   "source": "# Standard Python Libraries\nimport sys\nimport json\nimport warnings\nfrom datetime import timedelta\n\n# Data Manipulation and Analysis\nimport pandas as pd\nimport numpy as np\n\n# Data Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom snowflake.ml.modeling.xgboost import XGBRegressor, XGBClassifier\n\n# Snowpark Core\nfrom snowflake.snowpark import Session, DataFrame\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark.version import VERSION\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.exceptions import SnowparkSessionException\nfrom snowflake.snowpark.functions import (sproc, col, dayname, \n                              to_timestamp,min, max,split\n)\n\n\nfrom snowflake.snowpark import types as T\nfrom snowflake.snowpark.window import Window\n\n# Snowpark ML\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBRegressor\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.modeling.metrics import mean_absolute_percentage_error\nfrom snowflake.ml.registry import Registry\n\n# Snowflake Feature Store\nfrom snowflake.ml.feature_store import (\n    FeatureStore, FeatureView, Entity, CreationMode, setup_feature_store\n)\n\n# Snowflake Task API\nfrom snowflake.core import Root\nfrom snowflake.core.database import Database\nfrom snowflake.core.schema import Schema\nfrom snowflake.core.warehouse import Warehouse\nfrom snowflake.core.task import StoredProcedureCall\nfrom snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation\nfrom snowflake.core._common import CreateMode\n\n# Streamlit\nimport streamlit as st\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Create the Model Registry and register your initial model\nfrom snowflake.ml.registry import Registry\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5cfd9091-b782-450c-bbf2-b5a129b115af",
   "metadata": {
    "language": "python",
    "name": "session_start",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Create Snowflake Session object\nsession = get_active_session()\nsession.sql_simplifier_enabled = True\n\nsnowflake_environment = session.sql('SELECT current_user(), current_version()').collect()\nsnowpark_version = VERSION\n\n# Current Environment Details\nprint('\\nConnection Established with the following parameters:')\nprint('User                        : {}'.format(snowflake_environment[0][0]))\nprint('Role                        : {}'.format(session.get_current_role()))\nprint('Database                    : {}'.format(session.get_current_database()))\nprint('Schema                      : {}'.format(session.get_current_schema()))\nprint('Warehouse                   : {}'.format(session.get_current_warehouse()))\nprint('Snowflake version           : {}'.format(snowflake_environment[0][1]))\nprint('Snowpark for Python version : {}.{}.{}'.format(snowpark_version[0],snowpark_version[1],snowpark_version[2]))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b3a62b34-28b6-429b-90c8-ffc9e6f6a2ab",
   "metadata": {
    "name": "Data_Generation",
    "collapsed": false
   },
   "source": "Data Generation Script TBD"
  },
  {
   "cell_type": "markdown",
   "id": "ffcd2433-a352-47fb-879e-e8a9cfcf4535",
   "metadata": {
    "name": "EDA",
    "collapsed": false
   },
   "source": "- Do some basics EDA "
  },
  {
   "cell_type": "code",
   "id": "80563a67-0bba-45ef-8e59-fa9deb7e71fc",
   "metadata": {
    "language": "sql",
    "name": "transaction_df",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from transactions limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "84f93abc-053b-417b-9317-7c3db63e9776",
   "metadata": {
    "language": "sql",
    "name": "customer_df",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from customer_complaints limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2164b6c3-ce3f-45e1-97af-40f683d11c3d",
   "metadata": {
    "language": "sql",
    "name": "code_snippet",
    "collapsed": false
   },
   "outputs": [],
   "source": "--drop table fraud_analysis;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30f88cbc-8753-4871-b791-5520f8c36ed2",
   "metadata": {
    "name": "Merging_dataset",
    "collapsed": false
   },
   "source": "Merge two datasets"
  },
  {
   "cell_type": "code",
   "id": "a319d9c5-e621-4e8d-87bc-ba72f7203638",
   "metadata": {
    "language": "python",
    "name": "Merge_two_datasets",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Create the table using SQL\nsession.sql(\"\"\"\nCREATE OR REPLACE TABLE fraud_analysis AS\nSELECT \n    t.transaction_id, \n    t.customer_id, \n    t.transaction_amount, \n    t.is_fraud, \n    t.merchant_category,\n    t.device_type,\n    t.location,\n    t.transaction_time,\n    c.complaint_text, \n    c.keywords,\n    c.complaint_time\nFROM transactions t\nLEFT JOIN customer_complaints c\nON t.customer_id = c.customer_id\n\"\"\").collect()\n\n# Create a Snowpark DataFrame from the newly created table\ndf = session.table(\"fraud_analysis\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1043d068-7dee-49a2-9f7f-194d901858d2",
   "metadata": {
    "language": "sql",
    "name": "fA_list",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from fraud_analysis limit 2;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25ac6d56-1d1e-456e-a9d5-a0ef6401b37e",
   "metadata": {
    "language": "python",
    "name": "df_show"
   },
   "outputs": [],
   "source": "df.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9bc5a4ad-a867-4f58-b003-e3f38ad2fdc8",
   "metadata": {
    "name": "Adding_column_FE",
    "collapsed": false
   },
   "source": "- lets do feature engineering "
  },
  {
   "cell_type": "code",
   "id": "8a37c879-ba74-4923-aec7-244e23586b7e",
   "metadata": {
    "language": "sql",
    "name": "Adding_column",
    "collapsed": false
   },
   "outputs": [],
   "source": "-- ALTER TABLE fraud_analysis \n-- ADD COLUMN computed_sentiment STRING;\n-- UPDATE fraud_analysis \n-- SET computed_sentiment = SNOWFLAKE.CORTEX.SENTIMENT(complaint_text);\n-- SELECT complaint_text, computed_sentiment \n-- FROM fraud_analysis \n-- LIMIT 10;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f0d005a7-b1d6-4910-b9fa-aa22e3d2410e",
   "metadata": {
    "name": "Feature_Store",
    "collapsed": false
   },
   "source": "### Create features with Feature Store"
  },
  {
   "cell_type": "markdown",
   "id": "8346a407-0b50-4183-9b88-0760c88eaae9",
   "metadata": {
    "name": "cf_withfeaturestore",
    "collapsed": false
   },
   "source": "Initialize Feature Store\nLet's first create a feature store client. With CREATE_IF_NOT_EXIST mode, it will try to create a new Feature Store schema and all necessary feature store metadata if it doesn't exist already. It is required for the first time to set up a Feature Store. Afterwards, you can use FAIL_IF_NOT_EXIST mode to connect to an existing Feature Store.\n\nNote that the database being used must already exist. Feature Store will NOT try to create the database even in CREATE_IF_NOT_EXIST mode.\nGenerate cumulative behavioral metrics for users based on their transaction data, such as cumulative clicks and cumulative logins per hour. It involves the use of window functions and joins to combine and transform data from the CREDITCARD_TRANSACTIONS table."
  },
  {
   "cell_type": "code",
   "id": "62c231f3-cb38-42d1-82cd-01c3c7650cf9",
   "metadata": {
    "language": "python",
    "name": "see_timespan"
   },
   "outputs": [],
   "source": "df.select(min('TRANSACTION_TIME'), max('TRANSACTION_TIME'))\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "80c4a42d-0e07-4f86-83af-2f0c43d9eaf8",
   "metadata": {
    "language": "python",
    "name": "define_features",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "#Create a dict with keys for feature names and values containing transform code\nfrom snowflake.snowpark.functions import call_udf\nfeature_eng_dict = dict()\nfeature_eng_dict[\"TRANSACTION_TIME\"] = to_timestamp(\"TRANSACTION_TIME\")\nfeature_eng_dict[\"SENTIMENT_SCORE\"] = call_udf(\"SNOWFLAKE.CORTEX.SENTIMENT\", col(\"complaint_text\"))\nfeature_eng_dict[\"TRANSACTION_DAY\"] = dayname(col(\"transaction_time\"))\ndf = df.with_columns(feature_eng_dict.keys(), feature_eng_dict.values())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "132c4cb1-b816-4867-85f8-fe1ecdea7f03",
   "metadata": {
    "language": "python",
    "name": "df_show_v"
   },
   "outputs": [],
   "source": "df.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9fa62e2d-b2a2-4f7f-8e3b-30ef4f3b608f",
   "metadata": {
    "language": "python",
    "name": "df_explain",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "df.explain()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37e0bba9-1133-4e9e-8a6e-aa6dc2394c05",
   "metadata": {
    "name": "customertrans_entity",
    "collapsed": false
   },
   "source": "Creating Entities\n\nAn entity is an abstraction over a set of primary keys used for looking up feature data. An Entity represents a real-world \"thing\" that has data associated with it. Below cell registers an entity for Customer and Transaction in Feature Store"
  },
  {
   "cell_type": "code",
   "id": "2dd50344-d2fb-4ac8-adf0-78a6c3201ba2",
   "metadata": {
    "language": "python",
    "name": "define_feature_store"
   },
   "outputs": [],
   "source": "fs = FeatureStore(\n    session=session, \n    database=session.get_current_database(), \n    name=session.get_current_schema(), \n    default_warehouse=session.get_current_warehouse(),\n    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a277df89-db77-4b9a-a924-7b1221ac475c",
   "metadata": {
    "language": "python",
    "name": "list_entities"
   },
   "outputs": [],
   "source": "fs.list_entities()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d21b0b42-920f-4295-ac55-e1f2a928e734",
   "metadata": {
    "language": "python",
    "name": "load_or_register_entities"
   },
   "outputs": [],
   "source": "# First try to retrieve an existing entity definition, if not define a new one and register\ntry:\n    # Retrieve existing entity\n    customer_entity = fs.get_entity('CUSTOMER_ID_ENTITY') \n    print('Retrieved existing entity')\nexcept:\n    # Define new entity\n    customer_entity = Entity(\n        name = \"CUSTOMER_ID_ENTITY\",\n        join_keys = [\"CUSTOMER_ID\"],\n        desc = \"Features defined on a per customer level\")\n    \n    # Register\n    fs.register_entity(customer_entity)\n    print(\"Registered new entity\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cc5c788c-d46f-49e8-aebb-a2a78794465f",
   "metadata": {
    "language": "python",
    "name": "List_entities"
   },
   "outputs": [],
   "source": "fs.list_entities()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d1be891f-3225-46de-b688-b1f1c31981a4",
   "metadata": {
    "language": "python",
    "name": "create_feature_df"
   },
   "outputs": [],
   "source": "#Create a dataframe with just the ID, timestamp, and engineered features. We will use this to define our feature view\nfeature_df = df.select([\"CUSTOMER_ID\"]+list(feature_eng_dict.keys()))\nfeature_df.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e19f2b0d-4fc2-41a4-959f-a0d549e0d413",
   "metadata": {
    "name": "Feature_Views",
    "collapsed": false
   },
   "source": "# Using Feature Views\n\nA feature view is a group of logically-related features that are refreshed on the same schedule. The FeatureView constructor accepts a Snowpark DataFrame that contains the feature generation logic. The provided DataFrame must contain the join_keys columns specified in the entities associated with the feature view. In this example we are using time-series data, so we will also specify the timestamp column name.\n\nBelow cell creates a feature view for the customer features\n"
  },
  {
   "cell_type": "code",
   "id": "b7d79ea2-1b91-4c60-98a8-f54940097562",
   "metadata": {
    "language": "python",
    "name": "feature_veiw_creation"
   },
   "outputs": [],
   "source": "#define and register feature view\nfraud_fv = FeatureView(\n    name=\"FRAUD_FEATURES\",\n    entities=[customer_entity],\n    feature_df=feature_df,\n    timestamp_col=\"TRANSACTION_TIME\")\n\nfraud_fv = fs.register_feature_view(fraud_fv, version=\"1\", overwrite=True)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "451304eb-77ee-4e5c-a9f9-d05b25da33aa",
   "metadata": {
    "language": "python",
    "name": "feature_view"
   },
   "outputs": [],
   "source": "fraud_fv",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6b7067c5-5048-4a55-a462-31da4d337a10",
   "metadata": {
    "language": "python",
    "name": "show_feature_views"
   },
   "outputs": [],
   "source": "fs.list_feature_views()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "90323264-df3d-4679-ab8b-1c9ea3961120",
   "metadata": {
    "name": "Writeup_featurestore",
    "collapsed": false
   },
   "source": "This completes the setup for the Database objects and Feature Store Producer workflow. The data and the features which have been generated is available for the consumer with appropritate privileges. Time to head on to the next notebook!\nGenerating Datasets for Training\nWe are now ready to generate our training set. We'll define a spine DataFrame to form the backbone of our generated dataset and pass it into FeatureStore.generate_dataset() along with our Feature Views.\n\nNOTE: The spine serves as a request template and specifies the entities, labels and timestamps (when applicable). The feature store then attaches feature values along the spine using an AS-OF join to efficiently combine and serve the relevant, point-in-time correct feature data."
  },
  {
   "cell_type": "code",
   "id": "48a5bcb9-6d3b-4e26-b868-928fd05997ac",
   "metadata": {
    "language": "python",
    "name": "DF_SHOW"
   },
   "outputs": [],
   "source": "df.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c927fae-33fa-4760-a383-310e5119468c",
   "metadata": {
    "language": "python",
    "name": "generate_dataset"
   },
   "outputs": [],
   "source": "ds = fs.generate_dataset(\n    name=\"FRAUD_DETECTION_DATASET_V1\",\n    spine_df=df.drop(\"SENTIMENT_SCORE\", \"complaint_text\",\n \"TRANSACTION_DAY\",\"KEYWORDS\",\"COMPLAINT_TIME\"),\n    features=[fraud_fv],\n    spine_timestamp_col=\"TRANSACTION_TIME\",\n    spine_label_cols=[\"IS_FRAUD\"]\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f6a19d4-86ac-4118-b451-1014b0f5cf5c",
   "metadata": {
    "language": "python",
    "name": "convert_dataset_to_snowpark_and_pandas"
   },
   "outputs": [],
   "source": "ds_sp = ds.read.to_snowpark_dataframe()\nds_sp.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c0b22898-ddf1-41dc-ad03-b5a3ceaf9598",
   "metadata": {
    "language": "python",
    "name": "one_hot_encoding"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.types import StringType\nimport snowflake.ml.modeling.preprocessing as snowml\n\n# Select categorical columns (columns with StringType), excluding CUSTOMER_ID and TRANSACTION_ID\nOHE_COLS = [col.name for col in ds_sp.schema.fields\n            if isinstance(col.datatype, StringType)\n            and col.name not in ('CUSTOMER_ID', 'TRANSACTION_ID')]\n\n# Create output column names for one-hot encoding\nOHE_POST_COLS = [i + \"_OHE\" for i in OHE_COLS]\n\n# Encode categorical columns to numeric columns using OneHotEncoder\nsnowml_ohe = snowml.OneHotEncoder(input_cols=OHE_COLS, output_cols=OHE_POST_COLS, drop_input_cols=True)\n\n# Fit and transform the dataset\nds_sp_ohe = snowml_ohe.fit(ds_sp).transform(ds_sp)\n\n# Print the resulting column names\nds_sp_ohe.columns\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a60c1df-fda5-4990-a3d8-edd6b3e91f18",
   "metadata": {
    "name": "train_snowmlmodel",
    "collapsed": false
   },
   "source": "### training the model trying snowml model \n"
  },
  {
   "cell_type": "code",
   "id": "96297cac-0a0e-4bdc-8be6-162d8016ad0c",
   "metadata": {
    "language": "python",
    "name": "train_test_split"
   },
   "outputs": [],
   "source": "train, test = ds_sp_ohe.random_split(weights=[0.70, 0.30], seed=216)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4cd3338d-2cdf-4833-a699-17f6de8b1b96",
   "metadata": {
    "language": "python",
    "name": "fill_na"
   },
   "outputs": [],
   "source": "train = train.fillna(0)\ntest = test.fillna(0)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3f10cb5-867c-4dd0-a01e-d9ef12e80efb",
   "metadata": {
    "language": "python",
    "name": "train_show"
   },
   "outputs": [],
   "source": "train.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2ce9ce4-89c9-4177-8c1e-d41c9895289e",
   "metadata": {
    "language": "python",
    "name": "test_show"
   },
   "outputs": [],
   "source": "test.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b232775e-7e8b-4cc9-8002-d20b35e357af",
   "metadata": {
    "language": "python",
    "name": "define_themodel"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.xgboost import XGBRegressor, XGBClassifier\n\nsnow_xgb_tree = XGBClassifier(\n    input_cols=train.drop([\"IS_FRAUD\", \"TRANSACTION_TIME\", \"CUSTOMER_ID\",'TRANSACTION_ID']).columns,\n    label_cols=train.select(\"IS_FRAUD\").columns,\n    output_cols=\"FRAUD_PREDICTION\",\n    learning_rate = 0.75,\n    ##tree_method=\"exact\",\n    ##n_estimators=5,\n    booster = 'gbtree'\n)\n\nsnow_xgb_linear = XGBClassifier(\n    input_cols=train.drop([\"IS_FRAUD\", \"TRANSACTION_TIME\", \"CUSTOMER_ID\",\"TRANSACTION_ID\"]).columns,\n    label_cols=train.select(\"IS_FRAUD\").columns,\n    output_cols=\"FRAUD_PREDICTION\",\n    # tree_method=\"exact\",\n    # n_estimators=10,\n    booster = 'gblinear'\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ce24c71-dfdf-4f9c-b43d-217e0216504a",
   "metadata": {
    "language": "python",
    "name": "train_tree"
   },
   "outputs": [],
   "source": "\nsnow_xgb_tree.fit(train)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "465bab39-0b7d-4d9d-a485-9be40ecf1c34",
   "metadata": {
    "language": "python",
    "name": "train_linear"
   },
   "outputs": [],
   "source": "snow_xgb_linear.fit(train)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fd2afb56-86d6-46c4-aec1-09555869ba46",
   "metadata": {
    "language": "python",
    "name": "compute_predictions_and_perf_metrics"
   },
   "outputs": [],
   "source": "from sklearn.metrics import f1_score, precision_score, recall_score\ntest_preds_tree = snow_xgb_tree.predict(test).select([\"IS_FRAUD\", \"FRAUD_PREDICTION\"]).to_pandas()\ntest_preds_linear = snow_xgb_linear.predict(test).select([\"IS_FRAUD\", \"FRAUD_PREDICTION\"]).to_pandas()\n\nf1_tree = f1_score(test_preds_tree.IS_FRAUD, test_preds_tree.FRAUD_PREDICTION)\nf1_linear = f1_score(test_preds_linear.IS_FRAUD, test_preds_linear.FRAUD_PREDICTION)\n\n\nprecision_tree = precision_score(test_preds_tree.IS_FRAUD, test_preds_tree.FRAUD_PREDICTION)\nprecision_linear = precision_score(test_preds_linear.IS_FRAUD, test_preds_linear.FRAUD_PREDICTION)\n\nrecall_tree = recall_score(test_preds_tree.IS_FRAUD, test_preds_tree.FRAUD_PREDICTION)\nrecall_linear = recall_score(test_preds_linear.IS_FRAUD, test_preds_linear.FRAUD_PREDICTION)\n\n\nprint(f'GB Tree: \\n f1: {f1_tree} \\n precision {precision_tree} \\n recall: {recall_tree}')\nprint(f'GB Linear: \\n f1: {f1_linear} \\n precision {precision_linear} \\n recall: {recall_linear}')\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "66acb506-7931-4e01-9df5-af6b1df25111",
   "metadata": {
    "name": "sklearn_coding",
    "collapsed": false
   },
   "source": "### Sklearn code "
  },
  {
   "cell_type": "code",
   "id": "15fbb748-413b-4e8a-bdcc-ed4fedb09c17",
   "metadata": {
    "language": "python",
    "name": "sklearn_library"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n# Create the Model Registry and register your initial model\nfrom snowflake.ml.registry import Registry\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c15e5c83-b0e9-40a8-93dc-540468d54d99",
   "metadata": {
    "language": "python",
    "name": "sklearn_train_test"
   },
   "outputs": [],
   "source": "from sklearn.model_selection import train_test_split\nds_sp_pandas = ds_sp.to_pandas()\n\n# Assuming 'ds_sp' is a valid pandas DataFrame\nX = ds_sp_pandas[['DEVICE_TYPE', 'MERCHANT_CATEGORY', 'TRANSACTION_DAY', \n                  'TRANSACTION_AMOUNT', 'SENTIMENT_SCORE']]\ny = ds_sp_pandas['IS_FRAUD']\n\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Optional: Display shapes to verify the split\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "720010c4-fd6c-4e0d-b61f-01c3b0e40920",
   "metadata": {
    "language": "python",
    "name": "skllearn_pipeline_modelling"
   },
   "outputs": [],
   "source": "from sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\n\n# Define categorical and numerical columns\ncategorical_features = ['DEVICE_TYPE', 'MERCHANT_CATEGORY', 'TRANSACTION_DAY']\nnumerical_features = ['TRANSACTION_AMOUNT', 'SENTIMENT_SCORE']\n\n# Preprocessing pipeline\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', Pipeline([\n            ('imputer', SimpleImputer(strategy='median')),\n            ('scaler', StandardScaler())\n        ]), numerical_features),\n        ('cat', Pipeline([\n            ('imputer', SimpleImputer(strategy='most_frequent')),\n            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n        ]), categorical_features)\n    ]\n)\n\n\n# Define models\nmodels = {\n    \"XGBoost\": Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n    ]),\n    \"RandomForest\": Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', RandomForestClassifier())\n    ]),\n    \"LogisticRegression\": Pipeline([\n        ('preprocessor', preprocessor),\n        ('classifier', LogisticRegression())\n    ])\n}\n\nbest_model = None\nbest_score = 0\n\nfor name, model in models.items():\n    print(f\"Training {name} model...\")\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    score = accuracy_score(y_test, preds)\n    print(f\"{name} Accuracy: {score:.4f}\")\n\n    if score > best_score:\n        best_model = model\n        best_score = score\n\nprint(f\"Best model: {type(best_model.named_steps['classifier']).__name__} with accuracy {best_score:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dd079cea-1aa3-412f-b3a2-8e3712efba07",
   "metadata": {
    "name": "model_registry_md_snowml",
    "collapsed": false
   },
   "source": "# Logging the model to Model Registry\n   - We will log in Xgb_tree model & sklearn model "
  },
  {
   "cell_type": "code",
   "id": "1b682614-1012-4549-9e14-6ba5ddf65e7f",
   "metadata": {
    "language": "python",
    "name": "define_model_registry"
   },
   "outputs": [],
   "source": "from snowflake.ml._internal.utils import identifier\n#Create a snowflake model registry object \ndb = identifier._get_unescaped_name(session.get_current_database())\nschema = identifier._get_unescaped_name(session.get_current_schema())\n\nmodel_registry = Registry(session=session, \n                    database_name=session.get_current_database(), \n                    schema_name=session.get_current_schema(),\n                    options={\"enable_monitoring\": True})",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0467afc0-7926-4958-a748-58a3cfa86c2a",
   "metadata": {
    "language": "python",
    "name": "register_model_version_sklearn"
   },
   "outputs": [],
   "source": "#Deploy the sklearn model to the model registry\n# Define model name\nmodel_name = \"FRAUD_ANALYSIS\"\nversion_name = 'V1'\n\ntry:\n    mv = model_registry.get_model(model_name).version(version_name)\n    print(\"Found existing model version!\")\nexcept:\n    print(\"Logging new model version...\")\n    mv  = model_registry.log_model(\n        model_name=model_name,\n        model=best_model, \n        version_name=version_name,\n        sample_input_data=X_train,\n        comment = \"sklearn model build\",\n    )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "04e7dec9-6f5e-49c9-a72b-00c0280eb4d0",
   "metadata": {
    "language": "python",
    "name": "register_model_version_xgb"
   },
   "outputs": [],
   "source": "#Deploy the tree booster model to the model registry\n# Define model name\nmodel_name = \"FRAUD_ANALYSIS_XGB\"\ntree_version_name = 'V1'\n\ntry:\n    mv_tree = model_registry.get_model(model_name).version(tree_version_name)\n    print(\"Found existing model version!\")\nexcept:\n    print(\"Logging new model version...\")\n    mv_tree = model_registry.log_model(\n        model_name=model_name,\n        model=snow_xgb_tree, \n        version_name=tree_version_name,\n        comment = \"snow ml model built off feature store using tree booster\",\n    )\n    mv_tree.set_metric(metric_name=\"F1_score\", value=f1_tree)\n    mv_tree.set_metric(metric_name=\"Precision_score\", value=precision_tree)\n    mv_tree.set_metric(metric_name=\"Recall_score\", value=recall_tree)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4631a4bb-3fc1-4ca8-b0b0-a16192e4089a",
   "metadata": {
    "language": "python",
    "name": "model_registry_show"
   },
   "outputs": [],
   "source": "model_registry.show_models()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "385d1311-cebe-446f-acfd-44e894e96afd",
   "metadata": {
    "language": "python",
    "name": "model_registry_delete"
   },
   "outputs": [],
   "source": "## model_registry.delete_model(\"FRAUD_ANALYSIS\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41dc2320-6dab-402e-810d-1ac09ea60d62",
   "metadata": {
    "language": "python",
    "name": "show_model_versions"
   },
   "outputs": [],
   "source": "model_registry.get_model(\"FRAUD_ANALYSIS\").show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "835d0aba-fa5d-48e3-a6df-7645cac1032d",
   "metadata": {
    "language": "python",
    "name": "model_retrieving"
   },
   "outputs": [],
   "source": "### retrieving sklearn - frad model - version 1 \nreg_model = model_registry.get_model(\"FRAUD_ANALYSIS\").version(\"v1\")\n### retrieving sklearn - frad model - version 1 \nreg_model_tree = model_registry.get_model(\"FRAUD_ANALYSIS_XGB\").version(\"v1\")\n\nprint(reg_model)\nprint(reg_model_tree)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "916d25e0-cf07-4fef-a74b-6f85e3b56a50",
   "metadata": {
    "language": "python",
    "name": "print_model_version_and_metrics"
   },
   "outputs": [],
   "source": "print(mv)\nprint(mv.show_metrics())\n\nprint(mv_tree)\nprint(mv_tree.show_metrics())",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d21d856-83ac-4a48-8770-009c1677b769",
   "metadata": {
    "name": "intro_batch_inferencing",
    "collapsed": false
   },
   "source": "# Batch Inferencing on warehouse"
  },
  {
   "cell_type": "markdown",
   "id": "45871620-2429-451a-87f4-f6a2168f4ab1",
   "metadata": {
    "name": "batch_inferencing_tree_model",
    "collapsed": false
   },
   "source": "#### Two model what we have trained Sklearn & snowml lets predict using both the models "
  },
  {
   "cell_type": "code",
   "id": "54de8e92-d01d-4d65-88ed-bad264e36856",
   "metadata": {
    "language": "python",
    "name": "predict_from_snowml_model"
   },
   "outputs": [],
   "source": "reg_preds_tree = mv_tree.run(test, function_name = \"predict\")\nreg_probs_tree_prob = mv_tree.run(test, function_name=\"predict_proba\") \nreg_preds_tree.show(2)\nreg_probs_tree_prob.show(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3972da8d-3686-476f-83c3-41118c9a1de3",
   "metadata": {
    "language": "python",
    "name": "prediction_on_complete_dataset"
   },
   "outputs": [],
   "source": "### Extracting customer id, transaction Id, and prediction \n\nreg_probs_tree_prob_complete = mv_tree.run(ds_sp_ohe, \n                            function_name=\"predict_proba\").select(\"CUSTOMER_ID\", \n                        \"TRANSACTION_ID\", \"PREDICT_PROBA_0\", \"PREDICT_PROBA_1\" ) \n\nreg_probs_tree_prob_complete.show(3)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6d6b520-212a-4183-b4e0-62c02fdcc08a",
   "metadata": {
    "language": "python",
    "name": "Master_dataset"
   },
   "outputs": [],
   "source": "### Earlier data set before split \nds_sp.show(5)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9699f04b-a2de-433a-b505-a3e1831a30f9",
   "metadata": {
    "language": "python",
    "name": "cell5"
   },
   "outputs": [],
   "source": "schema = ds_sp.schema  # Correct: Access the schema as an attribute\nprint(schema)\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d0d861fb-6712-45d9-8301-b5212e3a49b6",
   "metadata": {
    "language": "python",
    "name": "cell6"
   },
   "outputs": [],
   "source": "schema = reg_probs_tree_prob_complete.schema  # Correct: Access the schema as an attribute\nprint(schema)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "05b8b92d-fa7e-4366-b6a4-a8923a156522",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "print(\"ds_sp columns:\", ds_sp.columns)\nprint(\"reg_probs_tree_prob_complete columns:\", reg_probs_tree_prob_complete.columns)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a66ad24e-41f3-4832-ab74-7ff9cf3d3a04",
   "metadata": {
    "language": "python",
    "name": "master_data_creation_python"
   },
   "outputs": [],
   "source": "joined_df = ds_sp.join(\n    reg_probs_tree_prob_complete,\n    on=[\"CUSTOMER_ID\", \"TRANSACTION_ID\"],\n    join_type=\"left\"\n)\n\njoined_df.show(2)\n# # Save the dataframe\n#joined_df.write.mode(\"overwrite\").save_as_table(\"FT_PREDICTION_FINAL\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "daac8a5a-9538-4ceb-ad4c-1c2773392bc1",
   "metadata": {
    "language": "python",
    "name": "master_data_creation_with_prediction"
   },
   "outputs": [],
   "source": "# # left join master dataset\n# # This dataset will be used for apps and other metrics \n\n# joined_df = ds_sp.join(\n#     reg_probs_tree_prob_complete,\n#     (ds_sp[\"CUSTOMER_ID\"] == reg_probs_tree_prob_complete[\"CUSTOMER_ID\"]) & \n#     (ds_sp[\"TRANSACTION_ID\"] == reg_probs_tree_prob_complete[\"TRANSACTION_ID\"]),\n#     join_type=\"left\",\n# )\n\n# # Prioritize columns from ds_sp and select all columns without renaming\n# selected_columns = [ds_sp[col] for col in ds_sp.columns] + \\\n#                    [reg_probs_tree_prob_complete[\"predict_proba_0\"], reg_probs_tree_prob_complete[\"predict_proba_1\"]]\n\n# # Use select to build dataframe with those specified columns and save the table\n# final_df = joined_df.select(*selected_columns)\n\n# #final_df.show(2)\n# # Save the dataframe\n# #final_df.write.mode(\"overwrite\").save_as_table(\"FT_Prediction\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "98270841-d5c9-4104-9673-b95d0c8c3a5f",
   "metadata": {
    "language": "sql",
    "name": "final_dataset"
   },
   "outputs": [],
   "source": "select * from FT_Prediction limit 2; ",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8147ad85-e574-4919-99cf-a538d5314390",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "# Get column information for both tables\ncols_1 = session.table(\"FT_Prediction\").columns\ncols_2 = session.table(\"FT_Prediction_v1\").columns\n\n# Print columns in first table but not in second\nprint(\"Columns in FT_Prediction but not in FT_Prediction_v1:\")\nprint([col for col in cols_1 if col not in cols_2])\n\n# Print columns in second table but not in first\nprint(\"Columns in FT_Prediction_1 but not in FT_Prediction:\")\nprint([col for col in cols_2 if col not in cols_1])\n\n# Print common columns\nprint(\"Common columns:\")\nprint([col for col in cols_1 if col in cols_2])",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4c8ae24c-bcaf-4c9e-982b-4fca8ab3b923",
   "metadata": {
    "name": "Prediction_using_sklearn",
    "collapsed": false
   },
   "source": "## Batch prediction using sk learn model "
  },
  {
   "cell_type": "code",
   "id": "b93efb51-814f-4181-9c4e-0fa892828bfe",
   "metadata": {
    "language": "python",
    "name": "predict_prob_from_sklearn_model"
   },
   "outputs": [],
   "source": "reg_probs_sk = mv.run(X_test, function_name=\"predict_proba\") \nreg_preds_sk = mv.run(X_test, function_name = \"predict\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "77cbd617-0694-425c-be51-a46c63fbcd93",
   "metadata": {
    "language": "python",
    "name": "print_prediction_sklearn"
   },
   "outputs": [],
   "source": "print(reg_probs_sk.head(2))\nprint(reg_preds_sk.head(2))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a4d6cc5-46b7-4511-aa0d-2d69e31df8fa",
   "metadata": {
    "name": "real_time",
    "collapsed": false
   },
   "source": "# Real time inferencing deployment on Snowpark Container Services"
  },
  {
   "cell_type": "code",
   "id": "96b16265-ab9a-400f-a1c8-ed6f013367e8",
   "metadata": {
    "language": "python",
    "name": "configuring_spcs"
   },
   "outputs": [],
   "source": "# Define model name\nmodel_name = \"FRAUD_ANALYSIS_SPCS\"\nimage_repo_name = \"AI_ML_REPO\"\ncp_name = \"AI_ML_CP\"\nnum_spcs_nodes = '3'\nspcs_instance_family = 'CPU_X64_L'\nservice_name = 'FRAUD_DETECTION_SERVICE'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d130b561-a6e6-4e6c-a07c-6adcf5234ea3",
   "metadata": {
    "language": "python",
    "name": "session_parameters"
   },
   "outputs": [],
   "source": "current_database = session.get_current_database().replace('\"', '')\ncurrent_schema = session.get_current_schema().replace('\"', '')\nextended_image_repo_name = f\"{current_database}.{current_schema}.{image_repo_name}\"\nextended_service_name = f'{current_database}.{current_schema}.{service_name}'",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4288ae01-2638-4b6d-8fe5-e69b1a5c0ee5",
   "metadata": {
    "language": "python",
    "name": "starting_session"
   },
   "outputs": [],
   "source": "session.sql(f\"alter compute pool if exists {cp_name} stop all\").collect()\nsession.sql(f\"drop compute pool if exists {cp_name}\").collect()\nsession.sql(f\"create compute pool {cp_name} min_nodes={num_spcs_nodes} max_nodes={num_spcs_nodes} instance_family={spcs_instance_family} auto_resume=True auto_suspend_secs=300\").collect()\nsession.sql(f\"describe compute pool {cp_name}\").show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2140b8f6-5445-48d2-834d-69cb66730a24",
   "metadata": {
    "language": "python",
    "name": "Repo_created"
   },
   "outputs": [],
   "source": "session.sql(f\"create image repository if not exists {extended_image_repo_name}\").collect()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "56eafa3d-3a09-4435-96c0-2d0e30323158",
   "metadata": {
    "language": "python",
    "name": "create_service_prediction",
    "collapsed": true,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "mv_tree.create_service(\n    service_name=extended_service_name,\n    service_compute_pool=cp_name,\n    image_repo=extended_image_repo_name,\n    ingress_enabled=True,\n    max_instances=int(num_spcs_nodes),\n    build_external_access_integration=\"ALLOW_ALL_INTEGRATION\"\n)\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d30c03fd-25a2-4dd6-9102-ff3009ecc031",
   "metadata": {
    "language": "python",
    "name": "List_services"
   },
   "outputs": [],
   "source": "mv_tree.list_services()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "640e472e-8ddf-45e5-aec7-4744006b04b0",
   "metadata": {
    "language": "python",
    "name": "Checing_service"
   },
   "outputs": [],
   "source": "session.sql(f\"SELECT VALUE:status::VARCHAR as SERVICESTATUS, VALUE:message::VARCHAR as SERVICEMESSAGE FROM TABLE(FLATTEN(input => parse_json(system$get_service_status('{service_name}')), outer => true)) f\").show(100)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9dce4c0d-2534-4ccb-a783-7c0f16e69fdf",
   "metadata": {
    "language": "python",
    "name": "prediction_url"
   },
   "outputs": [],
   "source": "session.sql(f\"show endpoints in service {service_name}\").collect()[0][\"ingress_url\"]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e68199c0-1d90-4aeb-b0a1-be2fa4f84c90",
   "metadata": {
    "language": "python",
    "name": "test_dataset"
   },
   "outputs": [],
   "source": "test.limit(1).show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "094602d7-2e7e-4e99-b92a-66dd3bf69015",
   "metadata": {
    "name": "Batch_inference",
    "collapsed": false
   },
   "source": "## Batch  inferencing using SPCS"
  },
  {
   "cell_type": "code",
   "id": "9a3f1814-3e2c-4a6f-9334-2dd8be4c6421",
   "metadata": {
    "language": "python",
    "name": "service_predict_predict"
   },
   "outputs": [],
   "source": "mv_tree.run(test.limit(4) , service_name=service_name, function_name=\"predict\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "32d4b9df-f59b-452e-a13c-f0f583c0d193",
   "metadata": {
    "language": "python",
    "name": "service_predict_prob"
   },
   "outputs": [],
   "source": "mv_tree.run(test.limit(4) , service_name=service_name, function_name=\"predict_proba\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc22a252-fe1d-4992-a3e1-9670dc87f375",
   "metadata": {
    "name": "feature_Importance",
    "collapsed": false
   },
   "source": "### Feature importance and score "
  },
  {
   "cell_type": "code",
   "id": "98ccdaa7-529a-4077-821f-b0b29055b8ce",
   "metadata": {
    "language": "python",
    "name": "feature_importance"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport matplotlib.pyplot as plt\n\n# Get feature names from preprocessor\nfeature_names = (models[\"RandomForest\"]\n                 .named_steps[\"preprocessor\"]\n                 .transformers_[0][2] +  # Numerical features\n                 list(models[\"RandomForest\"].named_steps[\"preprocessor\"]\n                      .transformers_[1][1]\n                      .named_steps[\"encoder\"]\n                      .get_feature_names_out(categorical_features)))  # Encoded categorical features\n\n# Get feature importance from the RandomForestClassifier\nimportances = best_model.named_steps[\"classifier\"].feature_importances_\n\n# Convert to DataFrame for better readability\nfeature_importance_df = pd.DataFrame(\n    {\"FEATURE\": feature_names, \"IMPORTANCE\": importances}\n).sort_values(by=\"IMPORTANCE\", ascending=False)\n\n# Print feature importances\n#print(feature_importance_df)\n\nsnowpark_df = session.create_dataframe(feature_importance_df)\n\n#snowpark_df\n#snowpark_df.write.mode(\"overwrite\").save_as_table(\"feature_importance_df\")\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96163fc5-a233-409e-b3e3-2a6c1e39259a",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "select Feature from feature_importance_df;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c1403f3-069d-4976-9e80-2ab386136a72",
   "metadata": {
    "language": "python",
    "name": "feature_importance_graph"
   },
   "outputs": [],
   "source": "# Plot feature importance\nplt.figure(figsize=(10, 5))\nplt.barh(feature_importance_df[\"Feature\"], feature_importance_df[\"Importance\"])\nplt.xlabel(\"Importance Score\")\nplt.ylabel(\"Features\")\nplt.title(\"Feature Importance for RandomForestClassifier Model\")\nplt.gca().invert_yaxis()\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e87e3371-a827-4f48-a125-edc883573b6d",
   "metadata": {
    "name": "model_explanability",
    "collapsed": false
   },
   "source": "# Model explainability"
  },
  {
   "cell_type": "code",
   "id": "4d66755b-5242-439c-85d9-b7169d45c5e8",
   "metadata": {
    "language": "python",
    "name": "explanability"
   },
   "outputs": [],
   "source": "shap_vals = mv_tree.run(test.sample(n=1000), function_name=\"explain\")\nshap_vals",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74fa9ff9-e267-42e9-8bf5-c85c55cf08c6",
   "metadata": {
    "language": "python",
    "name": "explanability_pandas"
   },
   "outputs": [],
   "source": "shap_pd = shap_vals.to_pandas()\nshap_pd.head(2)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2153f740-5205-4229-8344-2c54886b7bde",
   "metadata": {
    "language": "python",
    "name": "pip_install_shap"
   },
   "outputs": [],
   "source": "# !pip install shap",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dda276bf-7c00-4165-b478-9359a1766f3f",
   "metadata": {
    "language": "python",
    "name": "shap_lib",
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import shap \njust_shap = shap_pd.iloc[:, 10:]\njust_input_vals = shap_pd.iloc[:, :10].drop([\"CUSTOMER_ID\",\"IS_FRAUD\", \"TRANSACTION_TIME\"], axis=1)\n\nshap.summary_plot(np.array(just_shap), just_input_vals, feature_names = just_input_vals.columns)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5a8bc2c-3d5c-4311-a0d7-8b057bd43fd3",
   "metadata": {
    "language": "python",
    "name": "shap_plot"
   },
   "outputs": [],
   "source": "shap_pd.iloc[:, 10:].mean(axis=0).sort_values(ascending=False)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8caeee5-217f-4d3f-8143-48bd74854fe0",
   "metadata": {
    "language": "python",
    "name": "scatterplot"
   },
   "outputs": [],
   "source": "import seaborn as sns\n\nsns.scatterplot(data = shap_pd, x =\"LOAN_PURPOSE_NAME_Home purchase\", y = \"LOAN_PURPOSE_NAME_Home purchase_explanation\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3954c74b-db39-4198-810c-4ea078a9c3f5",
   "metadata": {
    "language": "python",
    "name": "seaborn"
   },
   "outputs": [],
   "source": "import seaborn as sns\n\nincome_0_to_1M = shap_pd[(shap_pd.INCOME>0) & (shap_pd.INCOME<1000000)]\nsns.scatterplot(data = income_0_to_1M, x =\"INCOME\", y = \"INCOME_explanation\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e6a667b7-4c9d-464a-a6b8-c20eacdcb427",
   "metadata": {
    "name": "Model_monitor_setup",
    "collapsed": false
   },
   "source": "## Model Monitoring setup"
  },
  {
   "cell_type": "markdown",
   "id": "95eab0fb-f8d7-4662-8ac1-509e899efc0e",
   "metadata": {
    "name": "Model_observability",
    "collapsed": false
   },
   "source": "#### do model drift \n#### observability \n### Explanability"
  },
  {
   "cell_type": "markdown",
   "id": "7069492f-3f4e-4536-940c-450bff967f0a",
   "metadata": {
    "name": "Monitor_Model_observability",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "ee7ca6fa-7d69-4fa9-b2d1-daeeab9fbf7a",
   "metadata": {
    "language": "sql",
    "name": "model_monitor_code"
   },
   "outputs": [],
   "source": "SELECT *\nFROM TABLE(MODEL_MONITOR_DRIFT_METRIC (\n    'FRAUD_ANALYSIS',            -- The name of your model to monitor\n    'accuracy_score',            -- The drift metric (e.g., accuracy_score, f1_score)\n    'TRANSACTION_AMOUNT',        -- The feature you want to monitor for drift (e.g., 'TRANSACTION_AMOUNT')\n    'DAY',                       -- Granularity (e.g., 'DAY', 'HOUR', 'MONTH')\n    '2025-01-01',                -- Start time (in 'YYYY-MM-DD' format)\n    '2025-03-01'                 -- End time (in 'YYYY-MM-DD' format)\n));\n\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "da14fc2a-f2dd-4f01-98c2-b4a3d31b735f",
   "metadata": {
    "language": "sql",
    "name": "model_task_code"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TASK MY_MODEL_TASK\n    WAREHOUSE = 'FRAUD_WH'  -- Specify your warehouse\n    SCHEDULE = 'USING CRON 0 0 * * * UTC'  -- Schedule it daily (adjust cron as needed)\nAS\n    CALL MONITOR_MODEL_PERFORMANCE();  -- Replace with your actual monitoring logic or stored procedure\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4ac66490-082d-4a9e-b4e7-d466c5aa4377",
   "metadata": {
    "language": "sql",
    "name": "create_model_monitor"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE MODEL MONITOR FRAUD_ANALYSIS_MONITOR\n    WITH\n        MODEL = 'FRAUD_ANALYSIS'\n        VERSION = 'V3'\n        FUNCTION = 'PREDICT'\n        SOURCE = 'TRANSACTIONS'\n        WAREHOUSE = 'MY_WAREHOUSE'\n        REFRESH_INTERVAL = '1 DAY'\n        AGGREGATION_WINDOW = '30 DAY'\n        TIMESTAMP_COLUMN = 'TRANSACTION_TIME'\n        BASELINE = 'initial_model_baseline'\n        ID_COLUMNS = ('CUSTOMER_ID')\n        PREDICTION_CLASS_COLUMNS = ('PREDICTED_IS_FRAUD')\n        ACTUAL_CLASS_COLUMNS = ('IS_FRAUD');\n",
   "execution_count": null
  }
 ]
}